\chapter{Implementation} \label{Implementation}

	This chapter delves into the implementation of key components that form the backbone of communication with the generator microservice, 
	which is responsible for handling model generation requests in Refinery's architecture.

	First I'll introduce the starting point of the project.
	Then I'll go into detail about how the backend creates a client, which communicates with the generator microservice. Finally I'll go into detail
	regarding the inner workings of the generator server.

	\section{Starting point} \label{Starting point}
		First, I will introduce the starting point of the project, so we get an overview, of how the backend initially worked regarding the model
		generations.

		The PushServiceDispatcher is the class responsibly for calling the functions of the ModelGenerationService instance, and thus initiating
		the generation related methods.

		\subsection{ModelGenerationService} \label{ModelGenerationService}
			The \textit{ModelGenerationService} is a core component of the application, designed to handle the initiation and management of
			model generation tasks on the backend server. This class encapsulates the logic for coordinating the execution of these 
			tasks, while providing a seamless interface for both initiating and canceling model generation processes.
			It is a singleton class, so from a generator point of view, we might consider this class to be the real dispatcher.

			The key features and responsibilities of the class include:
			\begin{itemize}
				\item{\textbf{Service Initialization:}} The \textit{ModelGenerationService} is a singleton, 
				ensuring a single, consistent instance throughout the application's lifecycle.
				It retrieves configuration parameters such as the model generation timeout 
				from the environment, defaulting to 600 seconds if not explicitly set.
				\item{\textbf{Model Generation Workflow:}} The main responsibility of the service is to execute model generation requests 
				by instantiating workers (\textit{ModelGenerationWorker}), using multithreading. 
				The workers perform the actual computational tasks:
				\begin{enumerate}
					\item The generateModel method is provided the PushWebDocumentAccess instance, which holds the model described by the partial modeling language
					of Refinery.
					\item A ModelGenerationWorker is instantiated and configured with the document state (provided by the PushWebDocumentAccess), 
					a random seed and a timeout value.
					\item The worker is started, as well as the resource heavy generation process. 
					The worker is started on a background thread. 
					The thread pool of for the generation workers can be configured via the 
					\textit{REFINERY\_MODEL\_GENERATION\_THREAD\_COUNT} environment variable.
				\end{enumerate}
				\item{\textbf{Cancellation Mechanism:}}
				During this process listed above, the service interacts with the document’s \textit{ModelGenerationManager} to monitor the worker's activity.
				The timeout and cancellation is handled via the use of cancelation tokens.
			\end{itemize}
			The use of dependency injection for worker provisioning ensures that the service is decoupled from the specific implementation details of task execution, making it easier to extend or modify in the future.

		\subsection{ModelGenerationWorker}
			The \textit{ModelGenerationWorker} class is the backbone of the backend’s model generation process. 
			It encapsulates the logic to handle the generation of computational models using a multithreaded approach, 
			ensuring responsiveness and reliability during execution. 
			Each instance of this class operates independently to process a specific model generation request.	

			The key features of the class include thread-safe execution, error management, scalable design via the use of the ExecutorService
			and the thread pool based multithreaded execution that it offers.

			The main responsibilites of the class are:
			\begin{itemize}
				\item{\textbf{Task Management:}} Each worker is uniquely identified by a UUID, allowing precise tracking and management of tasks.
				The worker implements the Runnable interface, enabling it to be executed as a separate thread in a thread pool.
				\item{\textbf{State Configuration:}} The worker’s state is set before execution via the setState method. This method configures the input document (PushWebDocument), random seed, and timeout duration required for the model generation process.
				The input text is extracted from the document and forms the basis of the model generation workflow.
				\item{\textbf{Model Generation Execution:}} The run method drives the core workflow. It first initializes a timeout mechanism and notifies the system that model generation has begun.
				The doRun method carries out the actual model generation, leveraging the following components:
				\begin{itemize}
					\item {\textbf{ProblemLoader:}} Parses the input text into a formal problem representation.
					\item {\textbf{ModelGenerator:}} Creates a generator instance to process the problem and produce a solution.
					\item {\textbf{MetadataCreator:}} Extracts metadata (nodes and relations) from the generated model for further processing.
					\item {\textbf{PartialInterpretation2Json:}} Converts intermediate results into a JSON format for downstream tasks.
				\end{itemize}
				\item{\textbf{Result Notification:}} Once a model is successfully generated, or if an error occurs, the worker notifies the document's precomputation listeners with the result. This ensures that other system components are informed of the status of the generation process.
				\item{\textbf{Cancellation Handling:}} The worker supports graceful cancellation through the cancel method. It uses a CancellationToken to periodically check whether the operation has been canceled and interrupts execution if necessary.
				A timeout mechanism automatically cancels the task if it exceeds the configured duration, ensuring that no worker consumes resources indefinitely.
			\end{itemize}

			The ModelGenerationWorker is the enabler of backend-driven computation.
			This implementation effectively delegates computationally intensive tasks, trying to minimize 
			the burden on client communication.

		\textbf{TODO!!! INSERT AN IMAGE OF AN UML DIAGRAM OF THE STARTING APPLICATION}

	\section{Generator Client} \label{Generator Client}
			Now that the deployed implementation has been been described, we can restructure our application, so that the 
			model generation is done via the generation microservice.

			The design of the \textit{ModelGenerationService} is scalable and modular:
			By delegating the actual computation to workers (ModelGenerationWorker), the service separates orchestration from execution, 
			allowing for future modifications. 
			
			This comes in handy, when implementing our client for communication with
			the generator server. We have to implement a client, which communicates generation requests towards the generator server,
			and upon completion, receives such generation results.

			\subsection{GeneratorWebSocketEndpoint}
			The client-side component responsible for sending generation requests and handling responses is implemented as the GeneratorWebSocketEndpoint class,
			leveraging Jetty's WebSocket API. This endpoint facilitates asynchronous communication with the generator microservice, 
			ensuring efficient request management over WebSocket connections. 

			In this section, I'll try to describe the key responsibilities and functionalities of the GeneratorWebSocketEndpoint class.
			\subsubsection{WebSocket Configuration} 
				The server, which the client is communicating with, can be configured.
				\begin{itemize}
					\item The WebSocket URI is dynamically determined based on environment variables (\textit{REFINERY\_GENERATOR\_WS\_HOST} 
					and \textit{REFINERY\_GENERATOR\_WS\_PORT}). If these are unset, default values (localhost and 1314) are used to ensure flexibility during deployment.
					\item The constructor initializes a WebSocketClient instance and sets default values, such as the worker's UUID and timeout duration.
				\end{itemize} 
			\subsubsection{Communication towards the server} 
				\begin{itemize}
					\item The client connects to the server via a ClientUpgradeRequest. The connection includes a custom header for the UUID of the worker.
					\item Sending the generation requests via sendGenerationRequest method. The method sends a structured JSON payload containing the request type,
					the UUID, the problem description, and a random seed. This start the model generation process on the server
					\item \label{clientcancel}Sending generation cancel requests via the sendCancelRequest method. This client sends a JSON, with the UUID of the generation to be 
					executed. Each generation opens a new session, with a new UUID for the server, so they can be uniquely idenfitied.
				\end{itemize} 
			\subsubsection{Receiving from the server} 
				Responses from the server are handled by the onText method, which processes various types of messages 
				(result, error, nodesMetadata, relationsMetadata, partialInterpretation), parsing them into appropriate data 
				structures and queuing them for consumption in their appropiate result queues.

			\subsubsection{Parsing responses from the queue} 
				The client maintains several LinkedBlockingQueue instances to manage received data:
				\begin{itemize}
					\item responseQueue: Stores results or errors associated with the generation process.
					\item nodesMetaDataQueue: Holds metadata about nodes in the generated model.
					\item relationsMetadataQueue: Queues relation metadata details.
					\item partialInterpretationQueue: Handles intermediate model interpretation data.
				\end{itemize} 
				The queues are accessed with timeout constraints to prevent indefinite blocking during retrieval. The timeout is set up by the class'
				timeout variable. 
				
				The LinkedBlockingQueue allowed the Jetty API's @WebSocketText annotated onText method, to put the received generation
				results into the queue, while the client is waiting for the results to arrive from the server. As soon as an element is put into the queue
				it can be popped from it. This way our code is safely waiting for the results
				to arrive from the server and no concurrency issues can arise. As I have already mentioned, each generation request instantiates a new
				client, so the size of these queues can only be between 0 and 1. No other generation client is accessing these queues of the instances.

			\subsubsection{Error Handling and Resource Management}
				\begin{itemize}
					\item The WebSocket lifecycle events (onOpen, onClose, onError) are implemented to handle connection state changes gracefully.
					\item Proper resource cleanup is ensured through the close method, which terminates the WebSocket session and stops the client instance.
					In normal operation, the client is the one initiating the closing of the connection.
					\item Errors during critical operations, such as connection establishment or closure, are logged and propagated as exceptions.
				\end{itemize} 

			\subsubsection{Scalability and Extensibility} 
				The GeneratorWebSocketEndpoint class is designed to support scalability.
				\begin{itemize}
					\item The GeneratorWebSocketEndpoint class is designed to support scalability.
					\item Its queuing mechanism ensures thread-safe handling of concurrent operations.
					\item The dynamic configuration allows seamless deployment in different environments without hardcoding server details.
					\item The modular structure facilitates extending the client to handle additional message types or features in the future.
				\end{itemize} 

			This implementation forms the core communication layer between the client and the generator microservice, enabling efficient 
			request handling, response parsing, and error management in a scalable and extensible manner.


		\subsection{ModelRemoteGenerationWorker}
			\subsubsection{Core functionality}
				The ModelRemoteGenerationWorker manages remote model generation tasks by leveraging a WebSocket client (GeneratorWebSocketEndpoint). 
				It sends model generation requests to a remote service, handles server responses, and retrieves metadata and interpretations required 
				for completing the generation process.
				By offloading the generation task to a remote service, this class allows the backend to handle computationally expensive operations 
				efficiently without overloading local resources. It preserves core mechanisms, such as timeout handling, task cancellation, 
				and error reporting, ensuring a robust and reliable workflow.

			\subsubsection{IGenerationWorker}
				For implementation, the IGenerationWorker interface was created, which defines a unified API for all generation workers. 
				This interface standardizes the core operations, including task initialization, execution, timeout management, and cancellation.
				This abstraction ensures that the backend can switch between local and remote workers without altering its core functionality.

				The key methods from the interface include:
				\begin{itemize}
					\item setState: Configures the worker with the input document, random seed, and timeout settings.
					\item start and startTimeout: Enqueues the worker for execution and schedules a timeout to enforce task completion within a defined duration.
					\item doRun: Executes the remote model generation logic by communicating with the external service.
					\item \label{workercancel}cancel: Cancels the task, ensuring both local and remote operations are halted gracefully.
				\end{itemize}

				The interface is a reusable API, which both the old ModelGenerationWorker, and our newly created ModelRemoteGenerationWorker implement.
				The implementation of this interface grants us plug-and-play functionality. The usage of the new generator microservice is interchangable
				with the old implementation. The backend can use either worker, based on the initial configuration, which can be set by an environment
				variable.
				The interface also allows for future workers to be implemented or a future hybrid functionality.

			\subsubsection{Workflow}
				The execution process of ModelRemoteGenerationWorker begins with setup and state configuration. 

				During task execution, the worker sends a generation request to the remote service by using a WebSocket client instance (GeneratorWebSocketEndpoint). 
				The server's responses are processed incrementally.

				The task completes successfully upon retrieving all necessary results, or it is terminated on errors, cancellation, or timeout.

				Timeouts and cancellations are managed via a ScheduledExecutorService, ensuring that the system remains responsive and resources are freed promptly. In case of errors, the worker captures exceptions, logs the issues, and notifies the backend with appropriate error results.

				The interaction between the GeneratorWebSocketEndpoint and the ModelRemoteGenerationWorker 
				showcases how client-side operations and backend service orchestration are seamlessly integrated to ensure efficient, scalable, and reliable processing.
				The 

			\subsubsection{Impact}
				By integrating remote model generation, this implementation reduces the computational load on the backend server, 
				making the system more scalable. The use of the IGenerationWorker interface allows a modular design, enabling seamless 
				transition between local and remote workers or future extensions. 
				
				Furthermore, this approach demonstrates how backend systems can leverage external services without significant architectural changes, 
				ensuring flexibility and scalability in modern distributed systems.

	\section{Generator Server} \label{Generator Server}
		Now that the client side of the generation process has been introduced, the server side implementation is what should be described next.
		The main idea behind this implementation, was to take the experiences of the local model generation worker, and basically implement everything that 
		is necessary for a generation to happen, in a multithreaded way, on our microservice. It is basically the implementation of the ModelGenerationWorker
		on a separate server.

		\subsection{GeneratorServerEndpoint} \label{GeneratorServerEndpoint}
			The GeneratorServerEndpoint is a WebSocket server endpoint that facilitates communication between remote clients 
			and the backend for model generation requests. This class plays a crucial role in enabling real-time, 
			bidirectional communication, supporting features like generation request handling, cancellation, and client disconnection.

			\subsubsection{Core Functionality} \label{Core Functionality}
				The two most important functionalities of the class are the handling of the generation requests and cancelation requests, which are
				 received over the 
				WebSocket sessions. 
				
				When a message of type "generationRequest" is received, the endpoint extracts generation details 
				such as the unique identifier (uuid), random seed, and problem description. The data is sent by the client via JSON, so extracting the
				needed key-value pairs is pretty straighforward. These are then forwarded to the ModelGeneratorDispatcher, 
				which manages the actual model generation task.

				\label{serverendpointcancel}For "cancel" messages, the endpoint instructs the ModelGeneratorDispatcher to cancel an ongoing generation task associated with the provided uuid.

				Any errors during the WebSocket communication are captured, logged.

				When a WebSocket session is closed, the endpoint notifies the ModelGeneratorDispatcher to clean up resources associated with the disconnected client.

			\subsubsection{Integration into the workflow} \label{Integration into the workflow}
				The GeneratorServerEndpoint serves as the entry point for remote clients into the backend's model generation system. 
				It relies on the ModelGeneratorDispatcher to manage generation and cancellation tasks. This provides a clean separation of concerns. 
				The design ensures that the endpoint focuses solely on communication, while the dispatcher handles the generation tasks.

			\subsubsection{Scalability and Extensibility} \label{Scalability and Extensibility}
				This WebSocket-based implementation allows the backend to support multiple concurrent client connections efficiently. 
				By using asynchronous communication, it ensures that tasks are queued and processed without blocking the ongoing generations. 
				The modular design makes it easy to extend functionality, such as adding new message types or enhancing error handling mechanisms.


		\subsection{ModelGeneratorDispatcher} \label{ModelGeneratorDispatcher}
			The ModelGeneratorDispatcher is a singleton class designed to manage and execute model generation requests on separate threads. 
			Its primary role is to coordinate the model generation tasks, making sure that they run concurrently and their status updates and results are 
			communicated back to clients over open WebSocket sessions. This dispatcher simplifies the handling of multiple requests 
			and helps with maintaining system consistency.

			\subsubsection{Key features} \label{Key features}
			The class implements the singleton design pattern to guarantee that only one instance of ModelGeneratorDispatcher exists throughout the application. 
			This is achieved via the use of a private constructor which prevents instantiation of the class from the outside, ensuring 
			that all interactions go through the singleton instance.
			This provides centralized control and avoids conflicting task management.


			Each model generation request is executed in a separate thread to prevent blocking the main application flow. 
			This is achieved through the ModelGeneratorExecutor class, which handles the task logic.
			The running model generations (ModelGenerationExecutor) are stored in a hashmap. The threads are identified by the UUID of the 
			generation (key).

			New generation requests are created, initialized and started on separate threads. They are then added to the previously mentioned hashmap.

			\label{servercanceldispatcher}Ongoing tasks can be cancelled via the UUID of the generation. As the hashmap stores these generation tasks identified by their UUID, it is fairly easy to do.

			Once a client has disconnected, the belonging task is cancelled, if not finished yet. The said task is removed from the hashmap. The UUID identification is 
			useful here aswell.

			The main methods of the class are:
			\begin{itemize}
				\item \textbf{getInstance:} Provides global access to the singleton instance of the dispatcher. Ensures thread-safe initialization using synchronized access.
				\item \textbf{addGenerationRequest:} First, dependencies are injected and a new ModelGeneratorExecutor is created for the request.
				Then the executor is initialized with task-specific details such as the random seed, problem string, and the WebSocket session with the client.
				Last, the executor thread is started and gets stored in the threadPool hashmap.
				\item \textbf{cancelGenerationRequest:} Retrieves the executor from the hashmap, based on the parameter UUID. Then the executor is signaled to
				cancel its operation.
				\item \textbf{disconnect:} Removes the executor from the threadPool and calls its disconnect method to release resources safely.
			\end{itemize}

		\subsection{ModelGeneratorExecutor}\label{ModelGenerationExecutor}
		The ModelGeneratorExecutor is a class responsible for performing the actual model generation tasks. It runs on a separate thread, 
		making it suitable for concurrent processing. This class does pretty much what the original ModelGenerationWorker did. 
		It even operates as part of a dispatcher system, executing generation tasks handed over by the ModelGeneratorDispatcher.

		\subsubsection{Key features and responsibilities}\label{Key features and responsibilites}
			The main responsibilites of the class are the following:
			\begin{itemize}
				\item \textbf{Initialization:} Before starting, it initializes the problem by loading the problem description, random seed, and WebSocket session details. These were
				received by the GeneratorServerEndpoint over WebSocket, and are given to an executor object by the ModelGeneratorDispatcher.
				\item \textbf{Task Execution (in run()):} The class handles the execution of the model generation. This is done in the exact same way, as it was done previously at the local ModelGenerationWorker.
				\item \textbf{Client communication:} By getting an open WebSocket session, the class instance can send generation state results, metadata and errors to the clients.
				\item \label{serverexecutorcancel}\textbf{Cancellation support:}The task can be interrupted or canceled at any point, ensuring that unnecessary processing is avoided. This is especially useful for timeout scenarios or user-initiated cancellations.
			\end{itemize}
			The key features of the class are:
			\begin{itemize}
				\item \textbf{Decoupled Execution:} Each ModelGeneratorExecutor instance works independently, processing a single request and reporting its status without interference from other tasks.
				\item \textbf{Feedback to the clients:} Task status updates are sent back to the dispatcher via the WebSocket session, ensuring real-time communication with clients.
				\item \textbf{Error Handling:} Any issues during validation or model generation are gracefully handled, with appropriate error messages sent to clients.
			\end{itemize}

		\subsection{ServerLauncher}\label{Serverlauncher}
		The ServerLauncher class serves as the entry point for starting a WebSocket server that 
		processes model generation requests. It listens on a configurable port, defaulting to 1314 
		unless overridden by the \textit{REFINERY\_GENERATOR\_WS\_PORT} environment variable. 
		
		The class sets up a Jetty server, configures a ServletContextHandler for managing requests and sessions, 
		and integrates WebSocket support through JettyWebSocketServletContainerInitializer.

		The GeneratorServerInitServlet servlet is registered for handling WebSocket communication.
		This basically just registers the GeneratorServerEndpoint.
		Once everything is configured, the server is started to handle incoming requests. 
		This class essentially establishes the server infrastructure for the application.

		\textbf{TODO!!!! A NICE IMAGE ABOUT THE GENERATOR SERVER ARCHITECTURE}

	\section{Containerization}\label{Containerization}
		Now that our application was running succesfully, with both the basic backend and the generator server working together in this 
		distributed architecture, came the containerization. 
		The new generator microservice was added 
		to the "run" task of the backend gradle project, and as such, the workings of the application could be tested locally. 
		
		Containerization was the next step in 
		updating our application. Containers help with the distribution and deployment of an application, so it was logical to handle this task as the next challenge.
		For the creation of the docker images some shell scripts were created. In this section, I will describe how those scripts handle the process.

		Keep in mind, that even before the restructuring, the build pipeline already existed. My main task was integrating the generator server docker image creation
		into these available scripts of the pipeline. I modified the prepare\_context.sh and the docker-bake.hcl files, and created a new file for the generator server
		image creation called Dockerfile.generator.

		\subsection{Preparation scripts}\label{Preparation scripts}
			The build.sh is the entrypoint to the building pipeline. First it calls a gradle build process, which builds all of our distributed projects 
			(like the basic backend
			and the newly created generator server projects), and returns us TAR archives of the projects' compiled dependencies and used classes.
			Then the prepare\_context.sh and bake.sh scripts are called.

			The prepare\_context.sh script organizes and optimizes build artifacts for Docker containerization. 
			It extracts the distribution archives, dissecting them into different distributed projects (like the backend and the generator), cleans up existing artifacts, 
			and separates application-specific JARs from shared dependencies. As a result common dependencies are deduplicated, and placed inside a common libs directory. 
			CPU architecture-specific JARs are placed in separate directories for AMD64 and ARM64 architectures. 
			This helps the container supports multi-architecture builds.
			Finally, all files are organized into a context directory for Docker image creation. The temporary distribution folders are removed.

			The bake.sh script builds and optionally pushes Docker images for the Refinery project using Docker Buildx and a docker-bake.hcl file. 
			It works in conjunction with a multi-stage Docker build process that is optimized for multiple architectures (x86\_64 and ARM64).

			The bake.sh script begins by retrieving the project version using get\_version.sh, which is exported as the REFINERY\_VERSION environment variable. 
			It also sets the REFINERY\_PUSH variable to false by default, allowing the user to control whether images are pushed to a container registry. 
			Then the script runs the docker buildx bake command with the docker-bake.hcl configuration file, 
			passing any additional arguments.


		\subsection{Docker images}\label{Docker images}
			The docker-bake.hcl specifies a group of targets (like the backend and the generator) and individual build configurations for each target. 

			The targets share a common base image defined in Dockerfile.base, which includes a slimmed-down JRE. 
			Architecture-specific dependencies are added as separate layers for AMD64 and ARM64, allowing efficient multi-architecture builds. 
			The targets specify platform compatibility, output configurations (including image naming and optional pushing), 
			and build contexts, referencing the base image.

			The Dockerfile.generator builds the generator image in stages, layering application-specific dependencies, startup scripts, 
			and JAR files on top of the base image defined in Dockerfile.base. It's implementation was heavily inspired by the Dockerfile.web backend image. 
			For the container, architecture-specific startup scripts and application binaries are added, 
			followed by application JARs. The final image is configured with environment variables (like the host and port for the server),
			a working directory, exposed ports for listening and an entry point to run the refinery-generator-server application.

			The finished images were pushed to the Docker Hub registry for my account. The public repository containing the images (backend,
			generator) can be accessible by anyone, and came in handy for the AWS deployment.

		TODO!!!!! IMAGE OF THE BUILD PIPELINE: Start->build.sh->gradle->preparecontext->docker bake

	\section{Deploying on AWS} \label{awsdeploy}
		As the docker images, desribed in section \ref{Docker images} were pushed to the remote Docker Hub registry, it was ready to be deployed on the AWS infrastructure, the 
		same way it was planned in section \ref{awsdecision}.

		I didn't want to deploy my project on the production AWS account, as asking for a user account with the appropriate access rights for the
		infrastructure might be a problem. Furthermore, it might pose a security risk to the production environment and even possible downtimes, if I
		were to make some mistakes during the setup of my environment.

		I registered a new AWS account with my own private email address. Luckily, Amazon provides a free-tier account for starters, where 
		I could deploy my application with virtually no associated costs. I used the AWS Management Console, which is the web GUI for 
		managing our AWS infrastructure.

		\subsection{Task definitions} \label{awstasks}
			First I set my tasks up. The tasks are basically the container instances of an application within the AWS ECS environment. 
			I created a task
			for the generator microservice called refinery-generator and the backend server called refinery-language-web.
			In the task configuration, I set the tasks to run on EC2 instances, provided the belonging docker registry link for the service,
			assigned the port mappings for the appropiate listening ports, and set the environment variables, 
			such as the generatos host and port's. The refinery-language-web task definition needed the "REFINERY\_GENERATOR\_WS\_HOST" to 
			be equal to the load balancer of the generator microservice's load balancer (more on that later, in section \ref{ALB NAT}).
			I set the networking mode for both tasks to be awsvpc, as they would be deployed on the same Virtual Private Cloud.
			Health checks also had to be implemented, so that our service knows, that the application is running and is in a stable state.
			For health checks, I used basic curl requests to the host and ports of the services. This caused some headaches in the 
			generator microservice deployment (\ref{healthcheck}).

		\subsection{Service definitions} \label{awsservices}
			Then I created a refinery cluster, with services running the defined tasks. The services are within an EC2 auto scaling group. 
			This allows us to scale our container instances, if they are heavily used, and the resource usage is over a threshold.
			For this I set up two treshholds:
			\begin{itemize}
				\item CPU usage: If the average CPU usage is over 70\% for more than 300 seconds, a new instance is deployed.
				\item Memory usage: If the average RAM usage is over 70\% for more than 300 seconds, a new instance is deployed.
			\end{itemize}
			I used Amazon's t3.micro EC2 instances for the auto scaling group, which are eligible for the free tier accounts. 
			They each have 2 vCPUs with 4 threads and 1GB of RAM.
		
		\subsection{Load Balancer definition} \label{awslb}
			An application load balancer (ALB) was created for the 
			backend and the generator server. The ALB would forward traffic to the running container instances within a target group, as longs as their
			health checks pass. 
			The ALB would be reachable for anyone on the internet on port 80. This port forwards traffic to the target group of the backend of the application.
			On port 81, the forwarding of the generator microservice requests is done towards the generator target group. However, this port is configured
			to accept requests only within the security group of the application. As such, only the backend can send requests to the microservice, hiding it 
			away from the public internet.
			I set the load balancing algorithm to the default round-robin, but least-used can also be a viable option.

	\section{Challenges during the implementation} \label{challenges}
	\subsection{Resource usage of development}
		The very first challenge that came up during the implementation was how heavy the resource usage of the project was.
		I wanted to develop on the Linux operating system, via the use of Windows Subsystem for Linux (WSL). I wanted to use linux
		because I could already see, that the most scripts for building had the extension "sh", which indicates a UNIX shell script.
		Windows (PowerShell or CMD) does not support those by default, hence why WSL was needed.

		When I initially opened the Refinery gradle project via a remote connection to my WSL, I was given an unwelcome suprise. The resource usage 
		for the gradle project indexing was incredible. My 8-core Ryzen 7 CPU was sitting at around 70\%  usage, alongside 
		16 gigabytes of my system memory being 92-99\% occupied. 
		
		The initial indexing of the Gradle project took around 45-60 minutes. During that time, the computer was pretty much unusable, as all of the resources
		were allocated for the gradle indexing task. However, to my surprise, even upon completion the resource usages didn't improve. The same amount of RAM
		was used by the WSL and there was no sign of it freeing up. 

		TODO insert image of CPU usage

		As a result of this, I have given up on the development using WSL. I developed the application on Windows, while using WSL only for the
		dockerization (\ref{Containerization}) of the finished applications (as that was needed for the running of the shell scripts).

	\subsection{Deduplification for docker images}
		The initial context preparation in section \ref{Preparation scripts} was implemented for two different distributions (the basic backend 
		named web, and a generator cli called cli). The common\_libs creation for both of these distributed images was very straightforward
		as the two distributions only had limited amount of JARs that they both used.

		With my project, a new distribution for the generator server was created. Originally I wanted to implement the deduplification in a way,
		where if two of the three distributions contained a JAR, the JAR would be put in the common\_libs folder.
		However, this implementation didn't seem too useful. In this case all of the resources needed for the web distribution were added to the common folder.
		As such, the shell script would have been needed to be modified, so that it could handle empty dist folders during the pipeline. On the other hand
		the files of the common library would be put in the base docker image, even tho it might not be needed for the running of our application.

		I decided to add only those resources to the common library, which are used in all of the created docker images. 
		As a result, the base image is not bloated and the shell script didn't need many modifications.

	\subsection{Health check for the generator server} \label{healthcheck}
		When deploying an application with the use of an Application Load Balancer, a target group of the tasks has to be specified.
		The load balancer needs to keep track of the running task docker container instances, so that it can decide, whether requests can be 
		forwarded to the running microservice. A health check has to be put in use, so that the load balancer can register the running instances.

		In my implementation of the generator server at section \ref{GeneratorServerEndpoint}, I only created a WebSocket server. However, Jetty 
		WebSocket servers won't respond to regular HTTP requests. With this original implementation, the health check for the load balancer would
		not work: it always signaled that the task was not running, as it wouldn't respond to the curl http requests towards the default path (/).

		As a workaround, I created a servlet called HealthCheckServlet, which responds to HTTP get requests at the path "/healthcheck",
		by returning a "Server works!" plain text.

		I added the servlet to the ServletContextHandler of the ServerLauncher class. By this, the load balancer can ensure the healthy state of the 
		the server instances, by sending a curl request to the "/healthcheck" path of the generator server.

	\subsection{Application Load Balancer and NAT} \label{ALB NAT}
		Just when I thought that the load balancer issues were solved in \ref{healthcheck}, more issues arose. I modified my frontend,
		so that it would communicate with my own AWS deployed backend infrastructure. However, generation requests still would not 
		work, with the requests resulting in a timeout, upon failing to connect with the generator microservice.

		I initallity thought this boils down to some kind of Access Control List (ACL) issue. I checked the ACL policies for the VPC, where all
		of my backend infrastructure was running. I allowed all inbound and outbound traffic in the VPC, just to insure that the VPC ACL policies
		aren't causing the issue. I also knew, that even though the VPC allows all traffic from the public internet, these policies can be overwritten
		with the usage of Security Groups, so that I can deny traffic from the public internet towards my generator microservice, and only allow
		inbound traffic coming form my backend server. To my surprise, this didn't resolve the issue either, the backend still couldn't connect
		with the generator server via the DNS of the ALB.

		To insure, that it is not an ACL related issue, I set both the ALB's and the services' security groups to allow all incoming and outbound traffic.
		I was gifted with the same error messages on both the backend task logs
		and on my frontend: connection couldn't be made to the generator microservice.

		Next I tried to check whether the EC2 instance, running the docker container (task) could reach the load balancer. I set up a 
		key pair in the AWS Management Console, and assigned it to the auto scaling group of the EC2 instances. By forcing a redeploy of my 
		backend service, I could finally SSH into the EC2 instances running the docker containers. By installing curl and ping on the instance
		I tried debugging network connectivity issues. The EC2 instance could send ICMP messages to the ALB, and it would respond. By sending
		curl messages to the health check path, that I created for the issue described in \ref{healthcheck}, the ALB would forward the traffic
		towards the generator server. The "Server works" plain text response was received by my EC2 instance running the backend.

		Lastly, I wanted to check, whether the container running inside the EC2 instance can reach the generator server. The task was 
		setup to run with the network mode awsvpc, so I assumed, that it should be able to reach the load balancer, as they are within the 
		same virtual private cloud. Since containers don't have anything installed, that is unnecessary for the running of the application,
		I copied a compiled static curl application, made for x86\_64 compatible computers. Running this static curl, I sent a GET request
		to the ALB, requesting the "/healthcheck" path, however, no response was received.

		Upon further research, I found out that ECS tasks (the containers) are not provided a public ip address by Amazon (only a private ipv4 addess). 
		The ALB is only granted a public address by AWS, and it cannot be given a private one. AWS doesn't perform Network Address Translation (NAT)
		for the endpoints within a Virtual Private Cloud, so I had to find a way, to perform the address translation, between the public and private 
		addresses.

		My two choices were either the setup of a Network Load Balancer (NLB), or a Network Address Translation (NAT) gateway. 
		
		The NLB uses the same target group methodology the
		ALB used, however NLBs support the use of private ipv4 addresses. I can set the NLB's target group to be the Application Load Balancer 
		with it's public address. This way the Network Load Balancer would act as a NAT gateway for the backend tasks.

		The NAT gateway would allow us to connect the services in public and private subnets, and would provide future extensibility, if more 
		features / tasks were to be added (even if some of them were not running in AWS).

		I decided to go with the Network Load Balancer, as the hourly rate of a NAT gateway is more expensive, than the hourly rate of a 
		Network Load Balancer \cite{natprice} \cite{nlbprice}. The hourly rate of a NAT gateway is \$0.046 dollars. The hourly rate of 
		the NLB is calculated more complicatedly. However, based on the example calculations provided on the AWS website (\cite{nlbprice}),
		with lower traffic the hourly rate would come down to nearly half of the price of the NAT gateway. I had to modify the task 
		configuration of the backend: I modified the "REFINERY\_GENERATOR\_WS\_HOST" environment variable to point to the DNS of the 
		Network Load Balancer. 

		TODO!!! IMAGE OF THE FINAL Infrastructure!!!!!
