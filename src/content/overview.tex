\chapter{Overview}
The goal of the project is to improve the scalability of the Refinery web-application. As such, we have to first examine
the current architecture of the application, and find places where it can be improved.
 
The scalability of an application can be greatly improved via the modification of the backend architecture, so that's naturally what
I'm going to describe first. After that, I'll go into detail about the different ways of how the backend could be restructured,
 so that we can achieve our end goal. 


\section{Backend}

The backend is a Java Jetty application, which connects to the clients through WebSocket. In the current implementation,
this connection is used to send information to the editor server, regarding the state of our model, based on the 
rule-defining partial modelling language of Refinery.
From now on, this editor server will be called XTextServer. Every modification of our graph problem definition
is sent to the XTextServer backend via the forementioned open WebSocket connection. If the user had finished the creation of the graph model
and the generate button is pressed on the client-side, a WebSocket request containing the modelgeneration service request is sent to the server 
The server then proceeds to generate the model based on the user input.

This generation task may take a significant amount of time, if more users were to use the service, so it would
 be wise to restructure our architecture in a way, where this generation happens on a separate server,
 acting similar to a microservice. This way, the load would be taken off from 
the XTextServer, and the newly created server would be the one responsible for the creation of the model.
 Let's call this future server GeneratorServer.

\section{Requirements}
Now that we have an idea, of what we have to perform to enhance the scalability of Refinery, we have to set some requirements for the
restructuring of the backend architecture and for the creation of the GeneratorServer service. 

The requirements for the restructuring of Refinary should be as follows:
\begin{enumerate}
        \item The backend of the application should be scalable 
        \item The client should be able to cancel the model generation
        \item The client must be able to show the state of the model generation  
        \item There must be a timeout set for the model generation
		\item (Optional) The implementation should allow the creation of other type of cliens,
		other than web browser-based clients
		\item (Optional) The editor and generator should be deployed together
\end{enumerate}

\section{Architectural designs in question}
As the requirements have been set, we can examine the different ways the generation of the model could be
moved to a separate server. In this section, I'll go into detail about the different architectures that came up during
the planning phase. Last, but not least, I'll explain, why I chose the WebSocket implementation afterall carefully researching
my possiblilites.

\subsection{REST API}
The first architecture that came to my mind, was a REST API. Using the Spring boot framework would allow for fast development
of our REST API. The state of the partial modelling editor would be sent to the REST API as a POST request, and the result
would be returned as a Json, which the client would parse, and output the model accordingly. However, it was still in question
how the generated model should be returned to the client.
\begin{itemize}
        \item \textbf{Long return:}
		The first implementation that came up, was a regular REST API, where the response of the API would only be received, after
		the model generation had been finished. As we know REST responses shouldn't take a long time to responde to received requests.
		The Generator is probably the slowest service of Refinery, so responding with the model details may take a fairly long time.
		As such, the purpose of a REST API would be invalidated / slow.
		\item \textbf{Long polling:}
		Here, the REST API would respond immediately with a "Starting generator service" message, had it received a request.
\end{itemize}
The limitation of both implementations is the fact that the request cannot be cancelled from the client side. As that is one 
of the main requirements of the architectural restructuring, this is a fact that cannot be looked over, so I concluded that 
another architecture should be looked at for the implementation.


\subsection{Remote Procedure Call}
The second architecture that came up during the planning phase was Remote Procedure Call (RPC). The generation is pretty much
a procedure, so calling it remotely sounded like a wise choice. Using gRPC would provide us the timeout logic, and other clients
could also be created in the future, as the generator server would function as a gRPC server. 
However, RPCs, such as gRPC work on request-response basis. As a result of this, some kind of workaround would have to be 
implemented in order to showcase the state of the generation on the client side, aswell as for the cancelation of the 
model generation request. The workaround for the state-showing and request cancelation would take
loads of effort. Maybe even a separate connection would have to be used for them. Wowever, the scalability of the
 application would not improve, compared to the final chosen architecture. 
As there are better options for the model generation server implementation, I didn't move forward with
the gRPC implementation for the generator server. 

\subsection{WebSocket server}
Last but not least, comes WebSocket. This basically would work the same way as the REST API long polling implementation,
 however, the connection would not be closed after the initial request. This is due to the way WebSocket works.
 WebSocket was originally invented for those usecases, where the clients
 had to continously poll the servers for results. That put unnecessary load on the servers. However, with WebSockets, the connection 
 is only closed, if chosen to. In our case, it means that after the model generation request is sent out by the client,
  the connection is kept alive, and the server may send back multiple responses over the same connection.
 This comes in handy for the representation of the state. The server can continously send to the client,
  where the model generation is currently at.

  The current implementation for the communication between the web client and the editor server is already using a WebSocket connection.
  As such, the same cancelation and timeout logic can be applied for the generator server, meaning that those two requirements would also be satisfied.
  After all of these considerations WebSocket seemed like the best implementation moving forward, but the way it would be implemented
  had still to be decided:

\begin{itemize}
        \item \textbf{Using a separate connections from the webclient side}
		This way, the webclient (or "frontend") would be the one making a connection with the generator server. This way
		there would be two connections created from the webclient: one towards the generator server, and a separate one 
		towards the editor server.

		\item \textbf{Using the same connection as the editor}
		In this implementation, the editor server would function as a "proxy" towards the generator server. This is made possible 
		by the fact that in the current implementation, the webclient already makes a WebSocket connection towards the editor server, and 
		the editor server is the one performing the generation.
\end{itemize}

After much consideration, I decided to go with the former implementation, due to the easier implementation, with no drawbacks
to the final scalability. As written previously, the editor server is communicating with the client over WebSocket, through which
the model generation requests are already sent over. This allows for less overall work, as the client off the application
doesn't have to be touched. The changes to the architecture of the application will be hidden from the client.

\subsection{Scalability}

Now that we have gone over the architectural changes that need to be implemented, we need to take a look at how the 
scalability of the application is really going to be improved.