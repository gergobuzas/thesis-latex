\chapter{Overview}
	The goal of the project is to improve the scalability of the Refinery web-application. 
	The scalability of an application is basically allowing our application to handle bigger loads and handle more simultaneous connections.
	This can mean reducing the operating costs and / or having better response times for each requests that the users of the application make. 

	As such, the current
	infrastructure has to be understood, and only then can ways be found, how improvements can be made.

	Scalability can be greatly improved via the modification of the backend architecture of an application, so that's naturally what
	I'm going to describe in greater detail. 

	After examining the current infrastructure and implementation of the graph generator, 
	I'll go into detail about the different approaches and ideas of how the backend could be restructured,
	so that we can achieve our goal in mind: improved scalability. 

\section{Frontend}
	The frontend of the Refinery web application is implemented using the React library. As a result, most of it is written in Typescript.
	As these are static websites, the production implementation is using Amazon's S3 buckets, as it provides a cheaper implementation
	than Amazon's EC2. 
	It basically works as a text editor, where the users can define the rules for their graph models, and see whether
	their rules are semantically / syntactically correct. If so, they can generate a model based on their defined rules
	via the press of a button, which sends the states of the text editor to the backend.

\section{Backend}

	The backend is a Java Jetty application. The clients connect to this server via WebSocket. 
	In the current implementation,
	this connection is used to send live information to the editor server, 
	regarding the state of the user-defined model.
	This state is based on what the user of the application wrote in the text editor in the
	rule-defining partial modelling language of Refinery. After each modification, only the differences
	from the last saved state are sent to the server.

	The server uses XText for keeping track of the state of the model. It is a library provided
	by Eclipse, and it makes the creation of domain specific languages (such as the one, used for Refinery)
	much easier. The most important usecase for XText, is the saving and verification of the text, 
	written in the editor.

	When a client (= an instance of the Refinery Frontend) makes a connection to backend server, a WebSocket 
	connection is made. Through this open WebSocket connection, the client continously sends JSON messages to
	the server.

	First, the initial update of the text is sent to the server.
	This inital update contains an example model, which also showcases for the user, how the language can be used
	for defining a graph problem. 
	
	After this initialization, only differences (e.g.: deltaText) are sent to the backend, so that 
	the edits made by the user can also be applied to the state on the backend session. 
	After each user edit/modification, the
	validity of the model is verified by the server: both semantically and syntactically, and sent back to the client.

	If no action is made by the user, the connection between the client and the server
	is kept alive via the use of the ping and pong messages of WebSocket.

	If the user finished the creation of the graph model and the generate button is pressed 
	on the client-side, a message containing the "modelGeneration" service request is sent to the server 
	The server then proceeds to generate the model based on the user input.

	This model generation may take a significant amount of time and many computing resources. 
	If vast amount of users were to use 
	the service at the same time, the increased generation times could lead to an unpleasing user experience and 
	new user connections might also be very slow. 
	
	As a result it would be wise to restructure the Refinery infrastructure in a way, 
	where the generation happens on a separate server,
	acting like a microservice. This way, the load would be taken off from 
	the basic backend of Refinery, and the newly created microservice would be the one responsible for
	the creation of the model.
	Our task is the creation of this generator microservice.

\section{Requirements}
Now that we have an idea, of what we have to perform to enhance the scalability of Refinery, we have to set some requirements for the
restructuring of the backend architecture and for the creation of the GeneratorServer service. 

The requirements for the restructuring of Refinary should be as follows:
\begin{enumerate}
        \item The backend of the application should be scalable 
        \item The client should be able to cancel the model generation
        \item The client must be able to show the state of the model generation  
        \item There must be a timeout set for the model generation
		\item (Optional) The implementation should allow the creation of other type of cliens,
		other than web browser-based clients
		\item (Optional) The editor and generator should be deployed together
\end{enumerate}

\section{Architectural designs in question}
As the requirements have been set, we can examine the different ways the generation of the model could be
moved to a separate server. In this section, I'll go into detail about the different architectures that came up during
the planning phase. Last, but not least, I'll explain, why I chose the WebSocket implementation afterall carefully researching
my possiblilites.

\subsection{REST API}
The first architecture that came to my mind, was a REST API. Using the Spring boot framework would allow for fast development
of our REST API. The state of the partial modelling editor would be sent to the REST API as a POST request, and the result
would be returned as a Json, which the client would parse, and output the model accordingly. However, it was still in question
how the generated model should be returned to the client.
\begin{itemize}
        \item \textbf{Long return:}
		The first implementation that came up, was a regular REST API, where the response of the API would only be received, after
		the model generation had been finished. As we know REST responses shouldn't take a long time to responde to received requests.
		The Generator is probably the slowest service of Refinery, so responding with the model details may take a fairly long time.
		As such, the purpose of a REST API would be invalidated / slow.
		\item \textbf{Long polling:}
		Here, the REST API would respond immediately with a "Starting generator service" message, had it received a request.
\end{itemize}
The limitation of both implementations is the fact that the request cannot be cancelled from the client side. As that is one 
of the main requirements of the architectural restructuring, this is a fact that cannot be looked over, so I concluded that 
another architecture should be looked at for the implementation.


\subsection{Remote Procedure Call}
The second architecture that came up during the planning phase was Remote Procedure Call (RPC). The generation is pretty much
a procedure, so calling it remotely sounded like a wise choice. Using gRPC would provide us the timeout logic, and other clients
could also be created in the future, as the generator server would function as a gRPC server. 
However, RPCs, such as gRPC work on request-response basis. As a result of this, some kind of workaround would have to be 
implemented in order to showcase the state of the generation on the client side, aswell as for the cancelation of the 
model generation request. The workaround for the state-showing and request cancelation would take
loads of effort. Maybe even a separate connection would have to be used for them. Wowever, the scalability of the
 application would not improve, compared to the final chosen architecture. 
As there are better options for the model generation server implementation, I didn't move forward with
the gRPC implementation for the generator server. 

\subsection{WebSocket server}
Last but not least, comes WebSocket. This basically would work the same way as the REST API long polling implementation,
 however, the connection would not be closed after the initial request. This is due to the way WebSocket works.
 WebSocket was originally invented for those usecases, where the clients
 had to continously poll the servers for results. That put unnecessary load on the servers. However, with WebSockets, the connection 
 is only closed, if chosen to. In our case, it means that after the model generation request is sent out by the client,
  the connection is kept alive, and the server may send back multiple responses over the same connection.
 This comes in handy for the representation of the state. The server can continously send to the client,
  where the model generation is currently at.

  The current implementation for the communication between the web client and the editor server is already using a WebSocket connection.
  As such, the same cancelation and timeout logic can be applied for the generator server, meaning that those two requirements would also be satisfied.
  After all of these considerations WebSocket seemed like the best implementation moving forward, but the way it would be implemented
  had still to be decided:

\begin{itemize}
        \item \textbf{Using a separate connections from the webclient side}
		This way, the webclient (or "frontend") would be the one making a connection with the generator server. This way
		there would be two connections created from the webclient: one towards the generator server, and a separate one 
		towards the editor server.

		\item \textbf{Using the same connection as the editor}
		In this implementation, the editor server would function as a "proxy" towards the generator server. This is made possible 
		by the fact that in the current implementation, the webclient already makes a WebSocket connection towards the editor server, and 
		the editor server is the one performing the generation.
\end{itemize}

After much consideration, I decided to go with the former implementation, due to the easier implementation, with no drawbacks
to the final scalability. As written previously, the editor server is communicating with the client over WebSocket, through which
the model generation requests are already sent over. This allows for less overall work, as the client off the application
doesn't have to be touched. The changes to the architecture of the application will be hidden from the client.

\subsection{Scalability}

Now that we have gone over the architectural changes that need to be implemented, we need to take a look at how the 
scalability of the application is really going to be improved.