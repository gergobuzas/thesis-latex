\chapter{Background}
\section{Refinery}
	Refinery \cite{refinery} is an open-source framework designed to automate the synthesis of diverse, consistent domain-specific graph models. 
	It features a high-level specification language that leverages partial models to concisely define a variety of graph generation challenges. 
	With its modern, cloud-based architecture, Refinery provides a scalable "Graph Solver as a Service" that applies logic-based reasoning rules to 
	efficiently produce diverse solutions by refining partial models. This framework supports applications such as system-level architecture synthesis,
	test generation for modeling tools, and traffic scenario synthesis for autonomous vehicles.

	Refinery is also hosted as a web application at "https://refinery.services" and can be used as a standalone service.
	This application allows the user to define their graph modelling problems using the specification language. 

\section{Java}
	Java \cite{java} is a widely-used, object-oriented programming language known for its platform independence, achieved through the Java Virtual Machine (JVM) 
	which allows Java applications to run on any device or operating system. This portability, combined with its robustness, scalability, and extensive libraries,
	makes Java a popular choice for developing enterprise-level applications, web services, and backend systems.

	Java supports core object-oriented programming principles like encapsulation, inheritance, and polymorphism, promoting code modularity and reusability.
	Additionally, Java’s memory management is handled through an automatic garbage collector, which improves performance by reclaiming memory used by objects 
	that are no longer accessible. The garbage collector (GC) also helps developers, so that they don't have to think about memory management when writing code in Java.
	With lower lever languages like C and C++, memory management has been a constant issue, as memory leakages worsen performance and software security can be hurt.
	Java also has a strong emphasis on security, providing a secure runtime environment through features such as bytecode 
	verification and sandboxing, which helps prevent unauthorized access and code manipulation.

	When working with Java, a Java Development Kit (JDK) has to be installed on the system.
	The Java Development Kit (JDK) is a distribution of Java technology by Oracle Corporation. 
	It implements the Java Language Specification (JLS) and the Java Virtual Machine Specification (JVMS) 
	and provides the Standard Edition (SE) of the Java Application Programming Interface (API). 

\section{Gradle}
	Gradle \cite{gradle} is a build automation tool for multi-language software development. It controls the development process in the tasks of compilation 
	and packaging to testing, deployment, and publishing. Supported languages include Java (as well as Kotlin, Groovy, Scala), C/C++, and JavaScript.
	Gradle builds on the concepts of Apache Ant and Apache Maven, and introduces a Groovy- and Kotlin-based domain-specific language contrasted with 
	the XML-based project configuration used by Maven. Gradle uses a directed acyclic graph to determine the order in which tasks can be run, 
	through providing dependency management. It runs on the Java Virtual Machine.

	Gradle was designed for multi-project builds, which can grow to be large. It operates based on a series of build tasks that can run serially or in parallel.
	Incremental builds are supported by determining the parts of the build tree that are already up to date; any task dependent only on those parts does not need 
	to be re-executed. It also supports caching of build components, potentially across a shared network using the Gradle Build Cache. 
	The software is extensible for new features and programming languages with a plugin subsystem. 

\section{Jetty}
	Eclipse Jetty \cite{jetty} provides a highly scalable and memory-efficient web server and servlet container, supporting many protocols
	 such as HTTP/3,2,1 and WebSocket.

\section{JSON}
	JSON (JavaScript Object Notation) \cite{json} is a lightweight data-interchange format. It is easy for humans to read and write. 
	It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language Standard ECMA-262 3rd Edition - December 1999.
	JSON is a text format that is completely language independent but uses conventions 
	that are familiar to programmers of the C-family of languages, including C, C++, \text{C\#}, Java, JavaScript, Perl, Python, and many others. These properties make JSON an ideal data-interchange language.

	JSON is built on two structures:

	\begin{itemize}
		\item A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, 
		keyed list, or associative array.
		\item An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.
	\end{itemize}

	These are universal data structures. Virtually all modern programming languages support them in one form or another. 
	It makes sense that a data format that is interchangeable with programming languages also be based on these structures.

	In JSON, they take on these forms:

	\begin{itemize}
		\item An object is an unordered set of name/value pairs.
		\item An object begins with left brace and ends with right brace.
		\item Each name is followed by colon and the name/value pairs are separated by comma.
	\end{itemize}

\section{REST API}
	A REST API \cite{restapi} (also called a RESTful API or RESTful web API) is an application programming interface (API) that conforms to the design principles of 
	the representational state transfer (REST) architectural style. REST APIs provide a flexible, lightweight way to integrate applications and to connect 
	components in microservices architectures.

	REST APIs communicate through HTTP requests to perform standard database functions like creating, reading,
	updating and deleting records (also known as CRUD operations) within a resource.

	REST API would use a GET request to retrieve a record. A POST request creates a new record. A PUT request updates a record, and a DELETE request deletes one. 
	All HTTP methods can be used in API calls. A well-designed REST API is similar to a website running in a web browser with built-in HTTP functionality.

\section{Spring}
	The Spring Framework \cite{springframework} provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind
	 of deployment platform.

	A key element of Spring is infrastructural support at the application level: Spring focuses on the 
	"plumbing" of enterprise applications so that teams can focus on application-level business logic, 
	without unnecessary ties to specific deployment environments.

	Spring Boot \cite{springboot} can be used for implementing enterprise-grade microservices and RESTful APIs, where the basic dependencies needed to run Spring
	are built into a starter project
	to improve the speed of the development process. Project specific dependencies and 3rd party libraries 
	can also be added during the creation of the starter project (Spring Initializr).
	By default Spring applications use Tomcat for the server communication, but Jetty and Undertow can also be configured.


\section{Remote Procedure Call}
	In distributed computing, a remote procedure call (RPC) \cite{rpc} is when a computer program causes a procedure (subroutine) to execute in a different address space 
	(commonly on another computer on a shared computer network), which is written as if it were a normal (local) procedure call, without the programmer 
	explicitly writing the details for the remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to 
	the executing program, or remote.
	This is a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response message passing system.

\section{gRPC}
	gRPC (gRPC Remote Procedure Calls) \cite{grpcwiki} is a cross-platform high-performance remote procedure call (RPC) framework. 
	gRPC was initially created by Google, but is open source and is used in many organizations. 
	Use cases range from microservices to the "last mile" of computing (mobile, web, and Internet of Things). 
	gRPC uses HTTP/2 for transport, Protocol Buffers as the interface description language, and provides features such as authentication,
	bidirectional streaming and flow control, blocking or nonblocking bindings, and cancellation and timeouts. 
	It generates cross-platform client and server bindings for many languages. Most common usage scenarios include connecting services in a microservices 
	style architecture, or connecting mobile device clients to backend services.

	In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, 
	making it easier for you to create distributed applications and services. As in many RPC systems, gRPC is based around the idea of 
	defining a service, specifying the methods that can be called remotely with their parameters and return types. \cite{grpcspec}
	
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[scale=0.8]{include/imgs/grpc_works.PNG}	
		\end{center}
		\caption{gRPC server and the clients \cite{grpcspec}}
	\end{figure}

	On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the
	client has a stub (referred to as just a client in some languages) that provides the same methods as the server.

	gRPC clients and servers can run and talk to each other in a variety of environments - from servers inside Google to your own desktop - 
	and can be written in any of gRPC’s supported languages. So, for example, you can easily create a gRPC server in Java with clients 
	in Go, Python, or Ruby. In addition, the latest Google APIs will have gRPC versions of their interfaces, letting you easily build Google functionality 
	into your applications.

	\subsection{Core concepts}

	Some of the core concepts of gRPC include \cite{grpcspec2}:
	\begin{itemize}
		\item \textbf{Unary RPC:} This is the simplest type of RPC. The client sends a single message and receives a single response. 
		\item \textbf{Server streaming RPC:} The server responds with multiple responses to the client. 
		After sending all its messages, the server’s status details (status code and optional status message) and optional trailing metadata are sent to the client.
		This completes processing on the server side. The client completes once it has all the server’s messages.
		\item \textbf{Client streaming RPC:} The client sends multiple messages and the server responds with one response message (along with its status details and optional trailing metadata). 
		\item \textbf{Bidirectional streaming RPC:} The call is initiated by the client invoking the method and the server receiving the client metadata, method name, and deadline. The server can choose to send back its initial metadata or wait for the client to start streaming messages.
		Client- and server-side stream processing is application specific. Since the two streams are independent, the client and server can read and write messages in any order.
		\item \textbf{Deadlines/Timeouts:} gRPC allows clients to specify how long they are willing to wait for an RPC to complete. On the server side, the server can query to see if a particular RPC has timed out, or how much time is left to complete the RPC.
		\item \textbf{RPC termination:} In gRPC, both the client and server make independent and local determinations of the success of the call, and their conclusions may not match. This means that, for example, you could have an RPC that finishes successfully on the server side (“I have sent all my responses!”) but fails on the client side (“The responses arrived after my deadline!”). It’s also possible for a server to decide to complete before a client has sent all its requests.
		\item \textbf{Cancelling an RPC:} Either the client or the server can cancel an RPC at any time. A cancellation terminates the RPC immediately so that no further work is done. The changes made before the cancellation are not rolled back.
	\end{itemize}

\section{WebSocket}
	The WebSocket Protocol \cite{websocket} enables two-way communication between a client
	running untrusted code in a controlled environment to a remote host
	that has opted-in to communications from that code.  The security
	model used for this is the origin-based security model commonly used
	by web browsers.  The protocol consists of an opening handshake
	followed by basic message framing, layered over TCP.  The goal of
	this technology is to provide a mechanism for applications that need two-way communication with servers that does
	not rely on opening multiple HTTP connections.

	\begin{figure}[h!]
		\includegraphics{include/imgs/http.PNG}
		\caption{HTTP connection}
	\end{figure}

	Historically, creating web applications that need bidirectional
	communication between a client and a server (e.g., instant messaging
	and gaming applications) has required an abuse of HTTP to poll the
	server for updates while sending upstream notifications as distinct
	HTTP calls.
	This results in a variety of problems:

	\begin{enumerate}
		\item The server is forced to use a number of different underlying TCP
		connections for each client: one for sending information to the
		client and a new one for each incoming message.
		\item The wire protocol has a high overhead, with each client-to-server
		message having an HTTP header.
		\item The client-side script is forced to maintain a mapping from the
		outgoing connections to the incoming connection to track replies.
	\end{enumerate}

	A simpler solution would be to use a single TCP connection for
	traffic in both directions.  This is what the WebSocket Protocol
	provides.

	\begin{figure}[h!]
		\includegraphics{include/imgs/websocket.PNG}
		\caption{WebSocket connection}
	\end{figure}

	\subsection{Connection lifecycle}
		The lifecycle of a WebSocket communication between a client and a server is as follows:
		\begin{enumerate}
			\item \textbf{The WebSocket communication starts with an upgrade HTTP request from the client to the server.}
			
			In the request's header the upgrade is set to be the WebSocket protocol, so that the server knows, that client wants to establish a WebSocket connection.
			The security version of the WebSocket protocol and a base64 encoded key (Sec-WebSocket-Key) are sent in this upgrade request. 
			\begin{figure}[h!]
				\begin{lstlisting}[language=bash]
				GET /xtext-service HTTP/1.1
				Host: refinery.services
				User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0
				Accept: */*
				Accept-Language: en-US,en;q=0.5
				Accept-Encoding: gzip, deflate, br, zstd
				Sec-WebSocket-Version: 13
				Origin: https://refinery.services
				Sec-WebSocket-Protocol: tools.refinery.language.web.xtext.v1
				Sec-WebSocket-Extensions: permessage-deflate
				Sec-WebSocket-Key: e6U+LNmInDXCSoVI7FswTQ==
				Connection: keep-alive, Upgrade
				Sec-Fetch-Dest: empty
				Sec-Fetch-Mode: websocket
				Sec-Fetch-Site: same-origin
				Pragma: no-cache
				Cache-Control: no-cache
				\end{lstlisting}
				\caption{Example WebSocket upgrade request}
			\end{figure}

			\item  \textbf{The server responds to the upgrade request.}

			The server acknowledges the upgrade by the client, and sends back the Sec-WebSocket-Accept.
			\begin{figure}[h!]
				\begin{lstlisting}[language=bash]
				HTTP/1.1 101 Switching Protocols
				Date: Mon, 11 Nov 2024 19:02:27 GMT
				Connection: upgrade
				Server: Jetty(12.0.14)
				Cache-Control: no-cache, no-store, max-age: 0, must-revalidate
				Expires: Thu, 01 Jan 1970 00:00:00 GMT
				Content-Security-Policy: default-src 'none'; script-src 'self' 'wasm-unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: blob:; font-src 'self'; connect-src 'self' data:; manifest-src 'self'; worker-src 'self' blob:;
				X-Content-Type-Options: nosniff
				X-Frame-Options: DENY
				Referrer-Policy: strict-origin
				Cross-Origin-Opener-Policy: same-origin
				Cross-Origin-Embedder-Policy: require-corp
				Cross-Origin-Resource-Policy: cross-origin
				Sec-WebSocket-Protocol: tools.refinery.language.web.xtext.v1
				Sec-WebSocket-Extensions: permessage-deflate
				Upgrade: websocket
				\end{lstlisting}
				\caption{Example WebSocket upgrade response (handshake)}
			\end{figure}

			\item \textbf{Communication between server and client}

			The communication is handled between the two connected endpoints via WebSocket frames. I will go into detail of the frames in section 2.10.2.

			\item \textbf{Either the server or the client initiates disconnect via a connection close frame}
		\end{enumerate}

	\subsection{WebSocket frames}
		WebSocket frames are the barebones of the communication between two connected endpoints. If the message we are trying to send is too big it is fragmented
		into multiple frames.
		A WebSocket frame consists of the following:
		\begin{itemize}
			\item FIN (1 bit): Indicates that it is a final fragment in a message
			\item RSV1-3 (3bits): Reserved bits
			\item Opcode (4bits): Indicates the message type. According to the RFC specification, the values indicate:
			\begin{itemize}
				\item 0x0 denotes a continuation frame
      			\item 0x1 denotes a text frame
				\item 0x2 denotes a binary frame
				\item 0x3-0x7 are reserved for further non-control frames
				\item 0x8 denotes a connection close
				\item 0x9 denotes a ping
				\item 0xA denotes a pong
				\item 0xB-F are reserved for further control frames
			\end{itemize}	
			\item Mask (1 bit): Indicates whether the payload data is masked by the masking key
			\item Payload length (7bits, 7+16bits or 7+64bits): The lenght of the payload
			\item Payload data 
		\end{itemize}
		The ping and pong frames are used for keeping the connection alive, if no communication is happening
		 between the endpoints otherwise. This can be considered as the "keep-alive" function of the protocol. If no pong answer is sent to the
		ping request, the connection is closed.
		

\section{Shell}
	In computing, a shell \cite{shell} is a computer program that exposes an operating system's services to a human user or other programs. 
	In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), 
	depending on a computer's role and particular operation. It is named a shell because it is the outermost layer around the operating system.

	Operating systems provide various services to their users, including file management, process management (running and terminating applications), batch processing, and operating system monitoring and configuration.

	Most operating system shells are not direct interfaces to the underlying kernel, even if a shell communicates with the user via peripheral devices 
	attached to the computer directly. Shells are actually special applications that use the kernel API in just the same way as it is used by 
	other application programs. A shell manages the user–system interaction by prompting users for input, interpreting their input, and then 
	handling output from the underlying operating system. In addition to shells running on local systems, there are different ways to make remote 
	systems available to local users; such approaches are usually referred to as remote access or remote administration.

\section{Shell script}
	A shell script \cite{shellscript} is a computer program designed to be run by a Unix shell, a command-line interpreter. The various dialects of shell scripts 
	are considered to be command languages. Typical operations performed by shell scripts include file manipulation, program execution, and printing text.
	The term is also used more generally to mean the automated mode of running an operating system shell.

\section{Cloud Computing}
	Cloud computing \cite{cloud} is the on-demand availability of computing resources (such as storage and infrastructure), as services over the internet. 
	It eliminates the need for individuals and businesses to self-manage physical resources themselves, and only pay for what they use.

	Cloud computing service models are based on the concept of sharing on-demand computing resources, software, and information over the internet. 
	Companies or individuals pay to access a virtual pool of shared resources, including compute, storage, and networking services, which are located on
	remote servers that are owned and managed by service providers. 

	One of the many advantages of cloud computing is that you only pay for what you use. This allows organizations to scale faster and more efficiently 
	without the burden of having to buy and maintain their own physical data centers and servers.  
	In simpler terms, cloud computing uses a network (most often, the internet) to connect users to a cloud platform where they request and access 
	rented computing services.

	There are three different cloud computing deployment models:
	\begin{itemize}
		\item \textbf{Public clouds} are run by third-party cloud service providers. They offer compute, storage, and network resources over the internet, 
		enabling companies to access shared on-demand resources based on their unique requirements and business goals.
		The most popular public cloud providers are Amazon, Google and Microsoft.
		\item \textbf{Private clouds} (also known as "on-premises" or "on-prem") are built, managed, and owned by a single organization 
		and privately hosted in their own data centers.
		They provide greater control, security, and management of data while still 
		enabling internal users to benefit from a shared pool of compute, storage, and network resources.
		\item \textbf{Hybrid clouds} are the mixture of using both public- and private clouds within the same organization.
	\end{itemize}

	There are three main types of cloud computing services:
	\begin{itemize}
		\item \textbf{Infrastructure as a service (IaaS)} offers on-demand access to IT infrastructure services, 
		including compute, storage, networking, and virtualization. It provides the highest level of control over your IT 
		resources and most closely resembles traditional on-premises IT resources.
		\item \textbf{Platform as a service (PaaS)} offers all the hardware and software resources needed for cloud 
		application development. With PaaS, companies can focus fully on application development without 
		the burden of managing and maintaining the underlying infrastructure.
		\item \textbf{Software as a service (SaaS)} delivers a full application stack as a service, from underlying 
		infrastructure to maintenance and updates to the app software itself. A SaaS solution is often an end-user application
		, where both the service and the infrastructure is managed and maintained by the cloud service provider.
	\end{itemize}

	\subsection{Cloud Native}
		Cloud native \cite{cloudnative} is the software approach of building, deploying, and managing modern applications 
		in cloud computing environments. Modern companies want to build highly scalable, flexible, and 
		resilient applications that they can update quickly to meet customer demands. 
		To do so, they use modern tools and techniques that inherently support application
		development on cloud infrastructure. These cloud-native technologies support fast 
		and frequent changes to applications without impacting service delivery, providing
		adopters with an innovative, competitive advantage.

	\subsection{Scaling}
		Scaling \cite{scaling} in cloud computing refers to the ability to increase or decrease IT resources as
		needed to meet changing demand. Scalability is one of the hallmarks of the cloud and the primary driver 
		of its exploding popularity with businesses. Scaling allows the businesses to pay only for the resources that
		they truly need.
		\begin{itemize}
			\item \textbf{Vertical scaling:} (also referred to as “scaling up” or “scaling down”) 
			You add or subtract power to an existing cloud server upgrading memory (RAM),
			storage or processing power (CPU).
			Usually this means that the scaling has an upper limit based on the capacity of the server
			or machine being scaled; scaling beyond that often requires downtime.
			\item \textbf{Horizontal scaling:} (also referred to as "scaling in" or "scaling out"), 
			You add more resources like servers to your system to spread out the workload across machines, 
			which in turn increases performance and storage capacity. 
			Horizontal scaling is especially important for businesses with high availability services requiring 
			minimal downtime.
		\end{itemize}

	\subsection{Load Balancing}
		Load balancing \cite{loadbalancing} is the practice of distributing computational workloads between two or more computers. 
		On the Internet, load balancing is often employed to divide network traffic among several servers. 
		This reduces the strain on each server and makes the servers more efficient, speeding up performance and 
		reducing latency. Load balancing is essential for most Internet applications to function properly. 
		By dividing user requests among multiple servers, user wait time is vastly cut down. 
		This results in a better user experience.

		Load balancing is handled by a tool or application called a load balancer. A load balancer can be 
		either hardware-based or software-based. 
		\begin{itemize}
			\item \textbf{Hardware load balancers} require the installation of a 
		dedicated load balancing device.
			\item \textbf{Software-based load balancers} can run on a server, on a virtual machine, 
		or in the cloud.
		\end{itemize}
		Content delivery networks (CDN) often include load balancing features.
		When a request arrives from a user, the load balancer assigns the request to a given server, 
		and this process repeats for each request. Load balancers determine which server should handle each request 
		based on a number of different algorithms. These algorithms fall into two main categories: static and dynamic.

		\begin{itemize}
			\item \textbf{Static load balancing} algorithms distribute workloads without taking into account the current 
			state of the system. A static load balancer will not be aware of which servers are performing 
			slowly and which servers are not being used enough. 
			Instead it assigns workloads based on a predetermined plan. 
			
			Static load balancing is quick to set up,
			but can result in inefficiencies. Round robin DNS and client-side random load balancing are 
			two common forms of static load balancing.
			\item \textbf{Dynamic load balancing} algorithms take the current availability, workload, 
			and health of each server into account. 
			They can shift traffic from overburdened or poorly performing servers to underutilized servers, 
			keeping the distribution even and efficient. However, dynamic load balancing is more difficult 
			to configure. A number of different factors play into server availability: 
			the health and overall capacity of each server, the size of the tasks being distributed, and so on.

			There are several types of dynamic load balancing algorithms, including least connection, weighted least connection, resource-based, and geolocation-based load balancing.
		\end{itemize}

\section{Amazon-related cloud technologies}
	\subsection{Amazon Web Services(AWS)}
		Amazon Web Services (AWS) is the public cloud offerring by the parent company Amazon. AWS has a vast offering
		of services, which help its end-users manage the infrastructures needed to host their own applications, 
		microservices, APIs and what not.
		AWS services are delivered to customers via a network of AWS server farms located throughout the world.

		In the following sections (2.14.2 - 2.14.6), I will go into detail what some of those services are and how
		they can be beneficial for the users of those services.

	\subsection{Amazon Elastic Compute Cloud (EC2)}
		Amazon Elastic Compute Cloud (Amazon EC2) \cite{ec2} provides on-demand, scalable computing capacity in the Amazon 
		Web Services (AWS) Cloud. Using Amazon EC2 reduces hardware costs so users can develop and deploy applications faster. 
		Amazon EC2 can be used to launch as many or as few virtual servers as needed, configure security and networking, 
		and manage storage. Capacity can be added (upscale) to handle compute-heavy tasks, such as monthly or yearly processes,
		or spikes in website traffic. When usage decreases, the capacity can be reduced (downscale) again.

		An EC2 instance is a virtual server in the AWS Cloud. When an EC2 instance is launched, the instance type that the 
		user specifies determines the hardware available to the instance. Each instance type offers a different balance of 
		compute, memory, network, and storage resources. Amazon EC2 provides the following high-level features:
		\begin{itemize}
			\item \textbf{Instances:} Virtual servers.
			\item \textbf{Amazon Machine Images (AMIs):} Preconfigured templates for the instances that package the components 
			needed for the server (including the operating system and additional software).
			\item \textbf{Instance types:} Various configurations of CPU, memory, storage, networking capacity, and graphics hardware for the instances.
			\item \textbf{Amazon EBS volumes:} Persistent storage volumes for the user's data using Amazon Elastic Block Store (Amazon EBS).
			\item \textbf{Instance store volumes:} Storage volumes for temporary data that is deleted when the user 
			stops, hibernates, or terminates their instance.
			\item \textbf{Key pairs:} Secure login information for user instances. AWS stores the public key and the user stores 
			the private key in a secure place.
			\item \textbf{Security groups:} A virtual firewall that allows users to specify the protocols, ports, and 
			source IP ranges that can reach their instances, and the destination IP ranges to which their instances can connect. 
		\end{itemize}

	\subsection{Amazon Elastic Container Service (ECS)}
		Amazon Elastic Container Service (Amazon ECS) \cite{ecs} is a fully managed container orchestration service that helps 
		you easily deploy, manage, and scale containerized applications. As a fully managed service, Amazon ECS comes 
		with AWS configuration and operational best practices built-in. It's integrated with both AWS and third-party tools, 
		such as Amazon Elastic Container Registry and Docker. This integration makes it easier for teams to focus on 
		building the applications, not the environment. You can run and scale your container workloads across AWS 
		Regions in the cloud, and on-premises, without the complexity of managing a control plane.

		There are three main layers in Amazon ECS:
		\begin{enumerate}
			\item \textbf{Capacity:} The infrastructure where the users runs their containers. Options include Amazon EC2, 
			AWS Fargate (a serverless, pay-as-you-go compute engine, where the server doesn't have to be managed for our service to run),
			and On-prem virtual machines and servers (infrastructure outside of Amazon) 
			\item \textbf{Controller:} The deployment and management of the applications that are run on the containers. The Amazon
			ECS scheduler is the software that manages / controls the applications.
			\item \textbf{Provisioning:} The tools used for interfacing with the ECS scheduler to deploy and manage the applications.
			and containers. Options include the AWS Management Console (Web GUI), AWS Command Line Interface (CLI), 
			AWS Software Development Kits (SDKs).
		\end{enumerate}

	\subsection{Virtual Private Cloud (VPC)}
		With Amazon Virtual Private Cloud (Amazon VPC) \cite{vpc}, you can launch AWS resources in a logically 
		isolated virtual network that you've defined. This virtual network closely resembles a traditional network 
		that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.

		Features include:
		\begin{itemize}
			\item \textbf{Subnets:} A subnet is a range of IP addresses in your VPC. A subnet must reside in a single Availability Zone. 
			After you add subnets, you can deploy AWS resources in your VPC.
			\item \textbf{IP addressing:} You can assign IP addresses, both IPv4 and IPv6, to your VPCs and subnets. 
			You can also bring your public IPv4 addresses and IPv6 GUA addresses to AWS and allocate them to resources in your VPC, 
			such as EC2 instances, NAT gateways, and Network Load Balancers.
			\item \textbf{Routing:} Use route tables to determine where network traffic from your subnet or gateway is directed.
			\item \textbf{Gateways and endpoints:} A gateway connects your VPC to another network. For example, use an internet gateway 
			to connect your VPC to the internet. Use a VPC endpoint to connect to AWS services privately, without the use of an 
			internet gateway or NAT device.
			\item \textbf{Peering connections:} Use a VPC peering connection to route traffic between the resources in two VPCs.
			\item \textbf{Traffic Mirroring:} Copy network traffic from network interfaces and send it to security and 
			monitoring appliances for deep packet inspection.
			\item \textbf{Transit gateways:} Use a transit gateway, which acts as a central hub, to route traffic between your VPCs, 
			VPN connections, and AWS Direct Connect connections.
			\item \textbf{VPC Flow Logs:} A flow log captures information about the IP traffic going to and from network interfaces in your VPC.
			\item \textbf{VPN connections:} Connect your VPCs to your on-premises networks using AWS Virtual Private Network (AWS VPN). 
		\end{itemize}

	\subsection{Elastic Load Balancing (ELB)}
		Elastic Load Balancing \cite{elb} automatically distributes your incoming traffic across multiple targets, 
		such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. 
		It monitors the health of its registered targets, and routes traffic only to the healthy targets. 
		Elastic Load Balancing scales your load balancer as your incoming traffic changes over time. 
		It can automatically scale to the vast majority of workloads.

		Elastic Load Balancing supports the following load balancers: 
		\begin{itemize}
			\item Application Load Balancers
			\item Network Load Balancers
			\item Gateway Load Balancers
			\item Classic Load Balancers
		\end{itemize}

		\subsubsection{Application Load Balancer}
			An Application Load Balancer \cite{elb} functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI)
			model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which 
			rule to apply, and then selects a target from the target group for the rule action. 
			You can configure listener rules to route requests to different target groups based on the content of the application 
			traffic. Routing is performed independently for each target group, even when a target is registered with multiple target groups. 
			You can configure the routing algorithm used at the target group level. 
			The default routing algorithm is round robin; alternatively, you can specify the least outstanding requests routing algorithm.

			You can add and remove targets from your load balancer as your needs change, without disrupting the overall 
			flow of requests to your application. Elastic Load Balancing scales your load balancer as traffic to your application changes over time. 
			Elastic Load Balancing can scale to the vast majority of workloads automatically.

			You can configure health checks, which are used to monitor the health of the registered targets so that the 
			load balancer can send requests only to the healthy targets.

	\subsection{Application Auto Scaling}
		Application Auto Scaling \cite{autoscale} is a web service for developers and system administrators 
		who need a solution for automatically scaling their scalable resources for individual AWS services beyond Amazon EC2. 

		Application Auto Scaling allows you to automatically scale your scalable resources according to conditions that you define.
		\begin{itemize}
			\item \textbf{Target tracking scaling} – Scale a resource based on a target value for a specific CloudWatch metric.
			\item \textbf{Step scaling} – Scale a resource based on a set of scaling adjustments that vary based on the size of the alarm breach.
			\item \textbf{Scheduled scaling} – Scale a resource one time only or on a recurring schedule.
		\end{itemize}

\section{Containers}
	Containerization \cite{container} is the packaging together of software code with all it’s necessary 
	components like 
	libraries, frameworks, and other dependencies so that they are isolated in their own "container."
	
	This is done so that the software or application within the container can be moved and run consistently 
	in any environment and on any infrastructure, independent of that environment or infrastructure’s 
	operating system. It’s basically a fully functional 
	and portable computing environment.

	The "lightweight" or portability characteristic of containers comes from their ability to share 
	the host machine’s operating system kernel, negating the need for a separate operating system for 
	each container and allowing the application to run the same on any infrastructure 
	even within virtual machines (VMs).

	Containerization and virtualization are similar in that they both allow for full isolation of 
	applications so that they can be operational in multiple environments. Where the main differences 
	lie are in size and portability (with containers being the more compact). 

	Containers are often used to package single functions that perform specific tasks—known as a microservice. 
	Microservices are the breaking up of the parts of an application into smaller, more specialized services. 
	This allows developers to focus on working on a specific area of an application, without impacting 
	the app’s overall performance. 

	\subsection{Docker}
	Docker is an open platform for developing, shipping, and running applications. \cite{docker}
	Docker provides the ability to package and run an application in a loosely isolated environment 
	called a container. The isolation and security lets you run many containers simultaneously 
	on a given host.

	Docker containers can be created by running docker images. Images are templates 
	containing the read-only instructions needed 
	to create the container running our application. To build an image, the user has to write a Dockerfile
	where they specify all of the steps needed to build their image (like linking libraries, exposing ports, 
	setting environment variables, etc.)

	The docker container is the running instance of the docker image. Those can be run by the use of the docker cli.


	\subsection{Docker compose}
	Docker Compose \cite{dockercompose} is a tool for defining and running multi-container applications. It is the key to unlocking a streamlined and efficient development and deployment experience.

	Compose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file. Then, with a single command, you create and start all the services from your configuration file.

	Compose works in all environments; production, staging, development, testing, as well as CI workflows.0
