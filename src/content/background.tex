\chapter{Background} \label{Background}
In this chapter I'll introduce the technologies and the phrases that need to be described for proper understanding of my thesis work.
Some of these technologies were only considered during the designing part of the thesis, while others were also present 
during the implementation.

\section{Java}
	Java \cite{java} is a widely-used, object-oriented programming language known for its platform independence, achieved through the Java Virtual Machine (JVM). 

	The backend of Refinery was programmed using the Java programming language. In the scope of this thesis, Java the main programming being used.

	Java supports core object-oriented programming principles like encapsulation, inheritance, and polymorphism, promoting code modularity and reusability.
	This comes in handy for the modification of the already existing codebase, while preserving the old implementaiton.

	When working with Java, a Java Development Kit (JDK) has to be installed on the system.
	The Java Development Kit (JDK) is a distribution of Java technology by Oracle Corporation. 
	The Refinery backend project uses JDK 21.

\section{Gradle}
	Gradle \cite{gradle} is the build automation tool used for the Refinery repository and it greatly helps the package management, 
	building, testing and the local deployment of the project / subprojects.

	Gradle supports Java as a programming language, which is the language in which the backend of Refinery was written.

	Gradle was designed for multi-project builds, like the Refinery project, consisting of several subprojects. 
	It operates based on a series of build tasks that can run serially or in parallel. This can come in handy for the backend and the generator service 
	running simultaneously, locally as separate subprojects.

	Incremental builds and caching of build components are supported, which provide faster build times during the development.

\section{Jetty}
	Eclipse Jetty \cite{jetty} provides a highly scalable and memory-efficient web server and servlet container, supporting many protocols
	 such as HTTP/3,2,1 and WebSocket. The original backend of Refinery uses Jetty as its server and could be a viable option for the future generation
	 microservice aswell.

\section{JSON}
	JSON (JavaScript Object Notation) \cite{json} is a lightweight data-interchange format. 

	JSON is built on two structures:
	\begin{itemize}
		\item A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, 
		keyed list, or associative array.
		\item An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.
	\end{itemize}

	The Refinery web application uses JSON messages as a way to communicate model generation request states, results and metadata between the frontend and the backend.

\section{Hypertext Transfer Protocol}
	HTTP (Hypertext Transfer Protocol) \cite{httpdoc} is an application layer protocol, designed for enabling communication between clients and servers. 
	HTTP works as a single request - single response protocol between the communicating endpoints.

	HTTP requests are human readable. They consist of the following parts:
	\begin{itemize}
		\item \textbf{Method:} Indicates what type of request the client wants to send to the server. The most used are the following:
		\begin{itemize}
			\item GET requests indicate resource fetching
			\item POST requests indicate resource upload 
			\item PUT / UPDATE requests indicate updating an existing resource
			\item DELETE requests indicate removal of an existing resource
		\end{itemize} 
		\item \textbf{Path:} The path of the resouce for which we send the request 
		\item \textbf{Version:} The version of the HTTP protocol
		\item \textbf{Headers:} Provide additional information for the server that is receiving the request. 
		\item \textbf{Body:} For some requests (e.g.: POST), additional info is provided here.
	\end{itemize}

\section{REST API}
	A REST API \cite{restapi} (also called a RESTful API or RESTful web API) is an application programming interface (API) that conforms to the design principles of 
	the representational state transfer (REST) architectural style. 

	REST API's via the use of the HTTP protocol utilize a single request - single response communication methodology.
	REST APIs communicate through HTTP requests to perform standard database functions like creating (POST), reading (GET),
	updating (PUT) and deleting (DELETE) records (also known as CRUD operations) within a resource.

	REST API's can be implemented really easily due to the efforts of many server framework providers. For Java, these include 
	the very popular Spring and Jetty frameworks. 

	At the start of the thesis, Refinery doesn't use or provide any REST APIs, but the implementation of one for the model 
	generation requests could be a viable option.

\section{Remote Procedure Call}
	In distributed computing, a remote procedure call (RPC) \cite{rpc} is when a computer program causes a procedure (subroutine) to execute in a different address space 
	(commonly on another computer on a shared computer network), which is written as if it were a normal (local) procedure call, without the programmer 
	explicitly writing the details for the remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to 
	the executing program, or remote.
	This is a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response message passing system.

	At the start of the thesis, Refinery doesn't use any RPC libraries, however the use of one (like gRPC) could be used for the model generation service implementation.

\section{gRPC}
	gRPC (gRPC Remote Procedure Calls) \cite{grpcwiki} is a cross-platform high-performance remote procedure call (RPC) framework. 
	gRPC uses HTTP/2 for transport.	It generates cross-platform client and server bindings for many languages (which includes Java). 

	\begin{figure}[h!]
		\begin{center}
			\includegraphics[scale=0.8]{include/imgs/grpc_works.PNG}	
		\end{center}
		\caption{gRPC server and the clients, as represented in gRPC documentation \cite{grpcspec}}
	\end{figure}

	In gRPC, a client application can directly call a method, which is defined on a server application on a different machine, as if it were a local object. 
	gRPC clients and servers can run and talk to each other in a variety of environments. This includes both on-prem, and more importantly in our case,
	cloud-based environments.
	\cite{grpcspec}

	\subsection{Core concepts}
	Some of the core concepts of gRPC include, but not limited to \cite{grpcspec2}:
	\begin{itemize}
		\item \textbf{Server streaming RPC:} The server responds with multiple responses to the client, which sends a single request. 
		After sending all its messages, the server’s status details (status code and optional status message) and optional trailing metadata are sent to the client.
		This completes processing on the server side. The client completes once it has all the server’s messages.
		\item \textbf{Deadlines/Timeouts:} gRPC allows clients to specify how long they are willing to wait for an RPC to complete. On the server side, the server can query to see if a particular RPC has timed out, or how much time is left to complete the RPC.
		\item \textbf{Cancelling an RPC:} Either the client or the server can cancel an RPC at any time. A cancellation terminates the RPC immediately so that no further work is done. The changes made before the cancellation are not rolled back.
	\end{itemize}

	The current implementation of Refinery doesn't use the gRPC framework. However, it can be a possible solution for the generator microservice implementaiton. 
	The remote method execution can lower the resource usage of the backend server.

\section{WebSocket}
	The WebSocket Protocol \cite{websocket} enables two-way communication between a client
	and a remote host.
	The goal of
	this technology is to provide a mechanism for applications that need two-way communication with servers that does
	not rely on opening multiple HTTP connections, but using one open connection, for the duration of the communication.

	The following part is taken from RFC 6455 \cite{websocket}, which describes why the creation of WebSocket procotol was needed.

	\textit{
		"Creating web applications using only HTTP, that need bidirectional
		communication between a client and a server has a major problem. 
		It requires the abuse of HTTP to poll the
		server for updates while sending upstream notifications as distinct
		HTTP calls.
		This results in a variety of problems:"
	}
	\begin{enumerate}
		\item \textit{"The server is forced to use a number of different underlying TCP
		connections for each client: one for sending information to the
		client and a new one for each incoming message."}
		\item \textit{"The wire protocol has a high overhead, with each client-to-server
		message having an HTTP header."}
		\item \textit{"The client-side script is forced to maintain a mapping from the
		outgoing connections to the incoming connection to track replies."}
	\end{enumerate}

	\begin{figure}[h!]
		\includegraphics{include/imgs/http.PNG}
		\caption{Regular HTTP connection}
	\end{figure}

	\textit{"A simpler solution would be to use a single TCP connection for
	traffic in both directions. This is what the WebSocket Protocol
	provides. A solution for these HTTP polling scenarios."}

	After the initial WebSocket handshake, the communication protocol is upgraded to WebSocket from HTTP.
	If no communicaiton is happening, the connection is kept alive between the two communicating endpoints via the use of ping and pong messages.
	If a ping sender endpoint doesn't receive a pong response, the connection is lost, and the endpoints are disconnected.
	Other than that, the protocol allows for full bidirectional real-time messaging to the endpoints.

	\begin{figure}[h!]
		\includegraphics{include/imgs/websocket.PNG}
		\caption{WebSocket connection over HTTP}
	\end{figure}

	The current implementation of Refinery already utilizes WebSocket. The frontend communicates editor changes and model generation
	states, results and metadata via the use of WebSocket. However, for the implementation of the generator microservice, it could also be a viable solution.

\section{Shell}
	In computing, a shell \cite{shell} is a computer program that exposes an operating system's services to a human user or other programs. 
	In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), 
	depending on a computer's role and particular operation. It is named a shell because it is the outermost layer around the operating system.

	In addition to shells running on local systems, there are different ways to make remote 
	systems available to local users; such approaches are usually referred to as remote access (like the use of SSH) or remote administration.

	Within the Refinery project, the use of a CLI based Unix shell (like bash) is needed for the execution of shell scripts. Furthermore, it can come in handy 
	as a remote access for our VMs via the use of SSH, which allows debugging our infrastructure.

\section{Shell script}
	A shell script \cite{shellscript} is a computer program designed to be run by a Unix shell, a command-line interpreter. Typical operations performed by shell scripts include file manipulation, program execution, and printing text.
	The term is also used more generally to mean the automated mode of running an operating system shell.

	The Refinery project has Unix shell scripts, which automate the build process and the containerization of the application.

\section{Cloud Computing}
	The following description is taken from a popular public cloud provider's, Google's cloud computing introduction:

	\textit{`Cloud computing \cite{cloud} is the on-demand availability of computing resources (such as storage and infrastructure), as services over the internet. 
	It eliminates the need for individuals and businesses to self-manage physical resources themselves, and only pay for what they use.'}

	\textit{`Cloud computing service models are based on the concept of sharing on-demand computing resources, software, and information over the internet. 
	Companies or individuals pay to access a virtual pool of shared resources, including compute, storage, and networking services, which are located on
	remote servers that are owned and managed by service providers.'}

	The infrastructure of Refinery is described as a public cloud deployment. 
	This means, that the infrastructure is run by third-party cloud service providers.
	The providers offer compute, storage and network resources over the internet. 

	The cloud services used for the deployment of Refinery are called `Infrastructure as a service (IaaS)'.
	IaaS service means \cite{cloud} \textit{`on-demand access to IT infrastructure services, 
		including compute, storage, networking, and virtualization. It provides the highest level of control over the IT 
		resources and most closely resembles traditional on-premises IT resources.'}

	\subsection{Cloud Native}
		This following definition of Cloud Native is taken from AWS's website \cite{cloudnative}:

		\textit{
			`Cloud native is the software approach of building, deploying, and managing modern applications 
			in cloud computing environments. Modern companies want to build highly scalable, flexible, and 
			resilient applications that they can update quickly to meet customer demands. 
			To do so, they use modern tools and techniques that inherently support application
			development on cloud infrastructure. These cloud-native technologies support fast 
			and frequent changes to applications without impacting service delivery, providing
			adopters with an innovative, competitive advantage.'
		}

	\subsection{Scaling}
		The following explanation of scaling is taken from VMWare's website \cite{scaling}:

		\textit{
			`Scaling  in cloud computing refers to the ability to increase or decrease IT resources as
			needed to meet changing demand. Scalability is one of the hallmarks of the cloud and the primary driver 
			of its exploding popularity with businesses. Scaling allows the businesses to pay only for the resources that
			they truly need.
			\begin{itemize}
				\item \textbf{Vertical scaling:} (also referred to as “scaling up” or “scaling down”) 
				You add or subtract power to an existing cloud server upgrading memory (RAM),
				storage or processing power (CPU).
				Usually this means that the scaling has an upper limit based on the capacity of the server
				or machine being scaled; scaling beyond that often requires downtime.
				\item \textbf{Horizontal scaling:} (also referred to as "scaling in" or "scaling out"), 
				You add more resources like servers to your system to spread out the workload across machines, 
				which in turn increases performance and storage capacity. 
				Horizontal scaling is especially important for businesses with high availability services requiring 
				minimal downtime.'
			\end{itemize}
		}
	\subsection{Load Balancing}
		The following parts regarding load balancing are taken from Cloudflare at \cite{loadbalancing}:

		\textit{
			`Load balancing \cite{loadbalancing} is the practice of distributing computational workloads between two or more computers. 
			On the Internet, load balancing is often used for dividing network traffic among several servers. 
			This reduces the strain on each server and makes the servers more efficient, speeding up performance and 
			reducing latency. Load balancing is essential for most Internet applications to function properly. 
			By dividing user requests among multiple servers, user wait time is vastly reduced. 
			This results in a better user experience.'
		}

		\textit{
			`Load balancing is handled by a tool or application called the load balancer. A load balancer can be 
			either hardware-based or software-based. 
			\begin{itemize}
				\item \textbf{Hardware load balancers} require the installation of a 
			dedicated load balancing device.
				\item \textbf{Software-based load balancers} can run on a server, on a virtual machine, 
			or in the cloud.
			\end{itemize}
			Content delivery networks (CDN) often include load balancing features.
			When a request arrives from a user, the load balancer assigns the request to a given server, 
			and this process repeats for each request. Load balancers determine which server should handle each request 
			based on a number of different algorithms. These algorithms fall into two main categories: static and dynamic.'
		}
		\begin{itemize}
			\item \textit{\textbf{Static load balancing} algorithms distribute workloads without taking into account the current 
			state of the system. A static load balancer will not be aware of which servers are performing 
			slowly and which servers are not being used enough. 
			Instead it assigns workloads based on a predetermined plan.}
			
			\textit{Static load balancing is quick to set up,
			but can result in inefficiencies. Round robin DNS and client-side random load balancing are 
			two common forms of static load balancing.}

			\item \textit{\textbf{Dynamic load balancing} algorithms take the current availability, workload, 
			and health of each server into account. 
			They can shift traffic from overburdened or poorly performing servers to underutilized servers, 
			keeping the distribution even and efficient. However, dynamic load balancing is more difficult 
			to configure. A number of different factors play into server availability: 
			the health and overall capacity of each server, the size of the tasks being distributed, and so on.}

			\textit{There are several types of dynamic load balancing algorithms, including least connection, weighted least connection, resource-based, and geolocation-based load balancing.}
		\end{itemize}

\section{Amazon-related cloud technologies}
	\subsection{Amazon Web Services(AWS)}
		Amazon Web Services (AWS) \cite{aws} is the public cloud offerring by the parent company Amazon. AWS has a vast offering
		of services, which help its end-users manage the infrastructures needed to host their own applications, 
		microservices, APIs and what not.
		AWS services are delivered to customers via a network of AWS server farms located throughout the world.

		The project uses Amazon Web Services for the infrastructure of Refinery.

		In the following sections (2.14.2 - 2.14.6), I will go into detail what some of those services are and how
		they can be beneficial for the users of those services.

	\subsection{Amazon Elastic Compute Cloud (EC2)}
		Amazon Elastic Compute Cloud (Amazon EC2) \cite{ec2} provides on-demand, scalable computing capacity in the Amazon 
		Web Services (AWS) Cloud. Using Amazon EC2 reduces hardware costs so users can develop and deploy applications faster. 
		Amazon EC2 can be used to launch as many or as few virtual servers as needed, configure security and networking, 
		and manage storage. Capacity can be added (upscale) to handle compute-heavy tasks, such as monthly or yearly processes,
		or spikes in website traffic. When usage decreases, the capacity can be reduced (downscale) again.

	\subsection{Amazon Elastic Container Service (ECS)}
		Amazon Elastic Container Service (Amazon ECS) \cite{ecs} is a fully managed container orchestration service that helps 
		the deployment, management, and scaling of containerized applications. It is integrated with both AWS and third-party tools, 
		such as Amazon Elastic Container Registry and Docker. This integration makes it easier for users to focus on 
		building the applications, not the environment. 

		There are three main layers in Amazon ECS:
		\begin{enumerate}
			\item \textbf{Capacity:} The infrastructure where the users runs their containers. Options include Amazon EC2, 
			AWS Fargate and On-prem virtual machines and servers (infrastructure outside of Amazon) 
			\item \textbf{Controller:} The deployment and management of the applications that are run on the containers. The Amazon
			ECS scheduler is the software that manages / controls the applications.
			\item \textbf{Provisioning:} The tools used for interfacing with the ECS scheduler to deploy and manage the applications.
			and containers. Options include the AWS Management Console (Web GUI), AWS Command Line Interface (CLI), 
			AWS Software Development Kits (SDKs).
		\end{enumerate}

		Amazaon ECS can be a valid option for the provisioning of Refinery's services. 

	\subsection{Virtual Private Cloud (VPC)}
		With Amazon Virtual Private Cloud (Amazon VPC) \cite{vpc}, users can launch AWS resources in a logically 
		isolated virtual network that is defined by the user. This virtual network closely resembles a traditional network 
		that the user would operate, with the benefits of using the scalable infrastructure of AWS.

	\subsection{Elastic Load Balancing (ELB)}
		Elastic Load Balancing \cite{elb} automatically distributes incoming traffic across multiple targets, 
		such as EC2 instances, containers, and IP addresses. 
		It monitors the health of its registered targets, and routes traffic only to the healthy targets. 
		Elastic Load Balancing scales load balancers as the incoming traffic changes over time. 

		Elastic Load Balancing supports the following load balancers: 
		\begin{itemize}
			\item Application Load Balancers
			\item Network Load Balancers
			\item Gateway Load Balancers
			\item Classic Load Balancers
		\end{itemize}

		\subsubsection{Application Load Balancer}
			An Application Load Balancer \cite{elb} functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI)
			model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which 
			rule to apply, and then selects a target from the target group for the rule action. 
			Routing is performed independently for each target group, even when a target is registered with multiple target groups. 
			Routing algorithm can be configured at the target group level. 
			The default routing algorithm is round robin; alternatively, the least outstanding requests routing algorithm can be used aswell.

			Health checks can be configured, which are used to monitor the health of the registered targets so that the 
			load balancer can send requests only to the healthy targets within a target group.

			The application load balancer can be utilized for the Refinery applications. This allows our scaling services (like the backend and 
			the generator) to receive requests based on the set load balancing algorithm. The services should be setup as separate target groups.

	\subsection{Application Auto Scaling}
		Application Auto Scaling \cite{autoscale} is a web service for developers and system administrators 
		who need a solution for automatically scaling their scalable resources for individual AWS services beyond Amazon EC2. 

		Application Auto Scaling allows automatic scaling of scalable resources according to conditions that are defined.

		The auto scaling is useful for the scaling of containerized Refinery services (backend, and generator service).

\section{Containers}
	Containerization \cite{container} is the packaging together of software code with all it’s necessary 
	components like 
	libraries, frameworks, and other dependencies so that they are isolated in their own container.
	
	This is done so that the software or application within the container can be moved and run consistently 
	in any environment and on any infrastructure, independent of that environment or infrastructure’s 
	operating system. It is a fully functional 
	and portable computing environment.

	The "lightweight" or portability characteristic of containers comes from their ability to share 
	the host machine’s operating system kernel, negating the need for a separate operating system for 
	each container and allowing the application to run the same on any infrastructure 
	even within virtual machines (VMs).

	Containers are often used to package single functions that perform specific tasks—known as a microservice. 
	Microservices are the breaking up of the parts of an application into smaller, more specialized services. 

	The services of the Refinery application should be developed with containerization in mind. This makes 
	the distribution and deployment of the application much easier.

	\subsection{Docker}
		Docker \cite{docker} is an open platform for developing, shipping, and running applications. 
		Docker provides the ability to package and run an application in a loosely isolated environment 
		called a container. The isolation and security lets you run many containers simultaneously 
		on a given host.

		Docker containers can be created by running docker images. Images are templates 
		containing the read-only instructions needed 
		to create the container running our application. To build an image, the user has to write a Dockerfile
		where they specify all of the steps needed to build their image (like linking libraries, exposing ports, 
		setting environment variables, etc.)

		The docker container is the running instance of the docker image. Those can be run by the use of the docker cli.

		The implemented Refinery services can be containerized via the use of Docker images. The images can be uploaded to the 
		Docker Registry, providing easier distribution.


	\subsection{Docker compose}
		Docker Compose \cite{dockercompose} is a tool for defining and running multi-container applications. 
		It is the key to unlocking a streamlined and efficient development and deployment experience.

		Compose simplifies the control of the entire application stack, making it easy to manage services, 
		networks, and volumes in a single, comprehensible YAML configuration file. 
		With a command, all of the services from the configuration file can be started.

		Docker compose can be used for the local deployment of the containerized Refinery backend and generator services.

