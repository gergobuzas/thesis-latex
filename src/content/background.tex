\chapter{Background}
\section{Refinery}
	Refinery \cite{refinery} is an open-source framework designed to automate the synthesis of diverse, consistent domain-specific graph models. 
	It features a high-level specification language that leverages partial models to concisely define a variety of graph generation challenges. 
	With its modern, cloud-based architecture, Refinery provides a scalable "Graph Solver as a Service" that applies logic-based reasoning rules to 
	efficiently produce diverse solutions by refining partial models. This framework supports applications such as system-level architecture synthesis,
	test generation for modeling tools, and traffic scenario synthesis for autonomous vehicles.

	Refinery is also hosted as a web application at "https://refinery.services" and can be used as a standalone service.
	This application allows the user to define their graph modelling problems using the specification language. 

\section{Java}
	Java \cite{java} is a widely-used, object-oriented programming language known for its platform independence, achieved through the Java Virtual Machine (JVM) 
	which allows Java applications to run on any device or operating system. This portability, combined with its robustness, scalability, and extensive libraries,
	makes Java a popular choice for developing enterprise-level applications, web services, and backend systems.

	Java supports core object-oriented programming principles like encapsulation, inheritance, and polymorphism, promoting code modularity and reusability.
	Additionally, Java’s memory management is handled through an automatic garbage collector, which improves performance by reclaiming memory used by objects 
	that are no longer accessible. The garbage collector (GC) also helps developers, so that they don't have to think about memory management when writing code in Java.
	With lower lever languages like C and C++, memory management has been a constant issue, as memory leakages worsen performance and software security can be hurt.
	Java also has a strong emphasis on security, providing a secure runtime environment through features such as bytecode 
	verification and sandboxing, which helps prevent unauthorized access and code manipulation.

	When working with Java, a Java Development Kit (JDK) has to be installed on the system.
	The Java Development Kit (JDK) is a distribution of Java technology by Oracle Corporation. 
	It implements the Java Language Specification (JLS) and the Java Virtual Machine Specification (JVMS) 
	and provides the Standard Edition (SE) of the Java Application Programming Interface (API). 

\section{Gradle}
	Gradle \cite{gradle} is a build automation tool for multi-language software development. It controls the development process in the tasks of compilation 
	and packaging to testing, deployment, and publishing. Supported languages include Java (as well as Kotlin, Groovy, Scala), C/C++, and JavaScript.
	Gradle builds on the concepts of Apache Ant and Apache Maven, and introduces a Groovy- and Kotlin-based domain-specific language contrasted with 
	the XML-based project configuration used by Maven. Gradle uses a directed acyclic graph to determine the order in which tasks can be run, 
	through providing dependency management. It runs on the Java Virtual Machine.

	Gradle was designed for multi-project builds, which can grow to be large. It operates based on a series of build tasks that can run serially or in parallel.
	Incremental builds are supported by determining the parts of the build tree that are already up to date; any task dependent only on those parts does not need 
	to be re-executed. It also supports caching of build components, potentially across a shared network using the Gradle Build Cache. 
	The software is extensible for new features and programming languages with a plugin subsystem. 

\section{Jetty}
	Eclipse Jetty \cite{jetty} provides a highly scalable and memory-efficient web server and servlet container, supporting many protocols
	 such as HTTP/3,2,1 and WebSocket.

\section{JSON}
	JSON (JavaScript Object Notation) \cite{json} is a lightweight data-interchange format. It is easy for humans to read and write. 
	It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language Standard ECMA-262 3rd Edition - December 1999.
	JSON is a text format that is completely language independent but uses conventions 
	that are familiar to programmers of the C-family of languages, including C, C++, \text{C\#}, Java, JavaScript, Perl, Python, and many others. These properties make JSON an ideal data-interchange language.

	JSON is built on two structures:

	\begin{itemize}
		\item A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, 
		keyed list, or associative array.
		\item An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.
	\end{itemize}

	These are universal data structures. Virtually all modern programming languages support them in one form or another. 
	It makes sense that a data format that is interchangeable with programming languages also be based on these structures.

	In JSON, they take on these forms:

	\begin{itemize}
		\item An object is an unordered set of name/value pairs.
		\item An object begins with left brace and ends with right brace.
		\item Each name is followed by colon and the name/value pairs are separated by comma.
	\end{itemize}

\section{REST API}
	A REST API \cite{restapi} (also called a RESTful API or RESTful web API) is an application programming interface (API) that conforms to the design principles of 
	the representational state transfer (REST) architectural style. REST APIs provide a flexible, lightweight way to integrate applications and to connect 
	components in microservices architectures.

	REST APIs communicate through HTTP requests to perform standard database functions like creating, reading,
	updating and deleting records (also known as CRUD operations) within a resource.

	REST API would use a GET request to retrieve a record. A POST request creates a new record. A PUT request updates a record, and a DELETE request deletes one. 
	All HTTP methods can be used in API calls. A well-designed REST API is similar to a website running in a web browser with built-in HTTP functionality.

\section{Spring}
	The Spring Framework \cite{springframework} provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind
	 of deployment platform.

	A key element of Spring is infrastructural support at the application level: Spring focuses on the 
	"plumbing" of enterprise applications so that teams can focus on application-level business logic, 
	without unnecessary ties to specific deployment environments.

	Spring Boot \cite{springboot} can be used for implementing enterprise-grade microservices and RESTful APIs, where the basic dependencies needed to run Spring
	are built into a starter project
	to improve the speed of the development process. Project specific dependencies and 3rd party libraries 
	can also be added during the creation of the starter project (Spring Initializr).
	By default Spring applications use Tomcat for the server communication, but Jetty and Undertow can also be configured.


\section{Remote Procedure Call}
	In distributed computing, a remote procedure call (RPC) \cite{rpc} is when a computer program causes a procedure (subroutine) to execute in a different address space 
	(commonly on another computer on a shared computer network), which is written as if it were a normal (local) procedure call, without the programmer 
	explicitly writing the details for the remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to 
	the executing program, or remote.
	This is a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response message passing system.

\section{gRPC}
	gRPC (gRPC Remote Procedure Calls) \cite{grpcwiki} is a cross-platform high-performance remote procedure call (RPC) framework. 
	gRPC was initially created by Google, but is open source and is used in many organizations. 
	Use cases range from microservices to the "last mile" of computing (mobile, web, and Internet of Things). 
	gRPC uses HTTP/2 for transport, Protocol Buffers as the interface description language, and provides features such as authentication,
	bidirectional streaming and flow control, blocking or nonblocking bindings, and cancellation and timeouts. 
	It generates cross-platform client and server bindings for many languages. Most common usage scenarios include connecting services in a microservices 
	style architecture, or connecting mobile device clients to backend services.

	In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, 
	making it easier for you to create distributed applications and services. As in many RPC systems, gRPC is based around the idea of 
	defining a service, specifying the methods that can be called remotely with their parameters and return types. \cite{grpcspec}
	
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[scale=0.8]{include/imgs/grpc_works.PNG}	
		\end{center}
		\caption{gRPC server and the clients \cite{grpcspec}}
	\end{figure}

	On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the
	client has a stub (referred to as just a client in some languages) that provides the same methods as the server.

	gRPC clients and servers can run and talk to each other in a variety of environments - from servers inside Google to your own desktop - 
	and can be written in any of gRPC’s supported languages. So, for example, you can easily create a gRPC server in Java with clients 
	in Go, Python, or Ruby. In addition, the latest Google APIs will have gRPC versions of their interfaces, letting you easily build Google functionality 
	into your applications.

	\subsection{Core concepts}

	Some of the core concepts of gRPC include \cite{grpcspec2}:
	\begin{itemize}
		\item \textbf{Unary RPC:} This is the simplest type of RPC. The client sends a single message and receives a single response. 
		\item \textbf{Server streaming RPC:} The server responds with multiple responses to the client. 
		After sending all its messages, the server’s status details (status code and optional status message) and optional trailing metadata are sent to the client.
		This completes processing on the server side. The client completes once it has all the server’s messages.
		\item \textbf{Client streaming RPC:} The client sends multiple messages and the server responds with one response message (along with its status details and optional trailing metadata). 
		\item \textbf{Bidirectional streaming RPC:} The call is initiated by the client invoking the method and the server receiving the client metadata, method name, and deadline. The server can choose to send back its initial metadata or wait for the client to start streaming messages.
		Client- and server-side stream processing is application specific. Since the two streams are independent, the client and server can read and write messages in any order.
		\item \textbf{Deadlines/Timeouts:} gRPC allows clients to specify how long they are willing to wait for an RPC to complete. On the server side, the server can query to see if a particular RPC has timed out, or how much time is left to complete the RPC.
		\item \textbf{RPC termination:} In gRPC, both the client and server make independent and local determinations of the success of the call, and their conclusions may not match. This means that, for example, you could have an RPC that finishes successfully on the server side (“I have sent all my responses!”) but fails on the client side (“The responses arrived after my deadline!”). It’s also possible for a server to decide to complete before a client has sent all its requests.
		\item \textbf{Cancelling an RPC:} Either the client or the server can cancel an RPC at any time. A cancellation terminates the RPC immediately so that no further work is done. The changes made before the cancellation are not rolled back.
	\end{itemize}

\section{WebSocket}
	The WebSocket Protocol \cite{websocket} enables two-way communication between a client
	running untrusted code in a controlled environment to a remote host
	that has opted-in to communications from that code.  The security
	model used for this is the origin-based security model commonly used
	by web browsers.  The protocol consists of an opening handshake
	followed by basic message framing, layered over TCP.  The goal of
	this technology is to provide a mechanism for applications that need two-way communication with servers that does
	not rely on opening multiple HTTP connections.

	\begin{figure}[h!]
		\includegraphics{include/imgs/http.PNG}
		\caption{HTTP connection}
	\end{figure}

	Historically, creating web applications that need bidirectional
	communication between a client and a server (e.g., instant messaging
	and gaming applications) has required an abuse of HTTP to poll the
	server for updates while sending upstream notifications as distinct
	HTTP calls.
	This results in a variety of problems:

	\begin{enumerate}
		\item The server is forced to use a number of different underlying TCP
		connections for each client: one for sending information to the
		client and a new one for each incoming message.
		\item The wire protocol has a high overhead, with each client-to-server
		message having an HTTP header.
		\item The client-side script is forced to maintain a mapping from the
		outgoing connections to the incoming connection to track replies.
	\end{enumerate}

	A simpler solution would be to use a single TCP connection for
	traffic in both directions.  This is what the WebSocket Protocol
	provides.

	\begin{figure}[h!]
		\includegraphics{include/imgs/websocket.PNG}
		\caption{WebSocket connection}
	\end{figure}

	\subsection{Connection lifecycle}
		The lifecycle of a WebSocket communication between a client and a server is as follows:
		\begin{enumerate}
			\item \textbf{The WebSocket communication starts with an upgrade HTTP request from the client to the server.}
			
			In the request's header the upgrade is set to be the WebSocket protocol, so that the server knows, that client wants to establish a WebSocket connection.
			The security version of the WebSocket protocol and a base64 encoded key (Sec-WebSocket-Key) are sent in this upgrade request. 
			\begin{figure}[h!]
				\begin{lstlisting}[language=bash]
				GET /xtext-service HTTP/1.1
				Host: refinery.services
				User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0
				Accept: */*
				Accept-Language: en-US,en;q=0.5
				Accept-Encoding: gzip, deflate, br, zstd
				Sec-WebSocket-Version: 13
				Origin: https://refinery.services
				Sec-WebSocket-Protocol: tools.refinery.language.web.xtext.v1
				Sec-WebSocket-Extensions: permessage-deflate
				Sec-WebSocket-Key: e6U+LNmInDXCSoVI7FswTQ==
				Connection: keep-alive, Upgrade
				Sec-Fetch-Dest: empty
				Sec-Fetch-Mode: websocket
				Sec-Fetch-Site: same-origin
				Pragma: no-cache
				Cache-Control: no-cache
				\end{lstlisting}
				\caption{Example WebSocket upgrade request}
			\end{figure}

			\item  \textbf{The server responds to the upgrade request.}

			The server acknowledges the upgrade by the client, and sends back the Sec-WebSocket-Accept.
			\begin{figure}[h!]
				\begin{lstlisting}[language=bash]
				HTTP/1.1 101 Switching Protocols
				Date: Mon, 11 Nov 2024 19:02:27 GMT
				Connection: upgrade
				Server: Jetty(12.0.14)
				Cache-Control: no-cache, no-store, max-age: 0, must-revalidate
				Expires: Thu, 01 Jan 1970 00:00:00 GMT
				Content-Security-Policy: default-src 'none'; script-src 'self' 'wasm-unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: blob:; font-src 'self'; connect-src 'self' data:; manifest-src 'self'; worker-src 'self' blob:;
				X-Content-Type-Options: nosniff
				X-Frame-Options: DENY
				Referrer-Policy: strict-origin
				Cross-Origin-Opener-Policy: same-origin
				Cross-Origin-Embedder-Policy: require-corp
				Cross-Origin-Resource-Policy: cross-origin
				Sec-WebSocket-Protocol: tools.refinery.language.web.xtext.v1
				Sec-WebSocket-Extensions: permessage-deflate
				Upgrade: websocket
				\end{lstlisting}
				\caption{Example WebSocket upgrade response (handshake)}
			\end{figure}

			\item \textbf{Communication between server and client}

			The communication is handled between the two connected endpoints via WebSocket frames. I will go into detail of the frames in section 2.10.2.

			\item \textbf{Either the server or the client initiates disconnect via a connection close frame}
		\end{enumerate}

	\subsection{WebSocket frames}
		WebSocket frames are the barebones of the communication between two connected endpoints. If the message we are trying to send is too big it is fragmented
		into multiple frames.
		A WebSocket frame consists of the following:
		\begin{itemize}
			\item FIN (1 bit): Indicates that it is a final fragment in a message
			\item RSV1-3 (3bits): Reserved bits
			\item Opcode (4bits): Indicates the message type. According to the RFC specification, the values indicate:
			\begin{itemize}
				\item 0x0 denotes a continuation frame
      			\item 0x1 denotes a text frame
				\item 0x2 denotes a binary frame
				\item 0x3-0x7 are reserved for further non-control frames
				\item 0x8 denotes a connection close
				\item 0x9 denotes a ping
				\item 0xA denotes a pong
				\item 0xB-F are reserved for further control frames
			\end{itemize}	
			\item Mask (1 bit): Indicates whether the payload data is masked by the masking key
			\item Payload length (7bits, 7+16bits or 7+64bits): The lenght of the payload
			\item Payload data 
		\end{itemize}
		The ping and pong frames are used for keeping the connection alive, if no communication is happening
		 between the endpoints otherwise. This can be considered as the "keep-alive" function of the protocol. If no pong answer is sent to the
		ping request, the connection is closed.
		

\section{Shell}
	In computing, a shell \cite{shell} is a computer program that exposes an operating system's services to a human user or other programs. 
	In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), 
	depending on a computer's role and particular operation. It is named a shell because it is the outermost layer around the operating system.

	Operating systems provide various services to their users, including file management, process management (running and terminating applications), batch processing, and operating system monitoring and configuration.

	Most operating system shells are not direct interfaces to the underlying kernel, even if a shell communicates with the user via peripheral devices 
	attached to the computer directly. Shells are actually special applications that use the kernel API in just the same way as it is used by 
	other application programs. A shell manages the user–system interaction by prompting users for input, interpreting their input, and then 
	handling output from the underlying operating system. In addition to shells running on local systems, there are different ways to make remote 
	systems available to local users; such approaches are usually referred to as remote access or remote administration.

\section{Shell script}
	A shell script \cite{shellscript} is a computer program designed to be run by a Unix shell, a command-line interpreter. The various dialects of shell scripts 
	are considered to be command languages. Typical operations performed by shell scripts include file manipulation, program execution, and printing text.
	The term is also used more generally to mean the automated mode of running an operating system shell.

\section{Cloud Computing}
	Cloud computing \cite{cloud} is the on-demand availability of computing resources (such as storage and infrastructure), as services over the internet. 
	It eliminates the need for individuals and businesses to self-manage physical resources themselves, and only pay for what they use.

	Cloud computing service models are based on the concept of sharing on-demand computing resources, software, and information over the internet. 
	Companies or individuals pay to access a virtual pool of shared resources, including compute, storage, and networking services, which are located on
	remote servers that are owned and managed by service providers. 

	One of the many advantages of cloud computing is that you only pay for what you use. This allows organizations to scale faster and more efficiently 
	without the burden of having to buy and maintain their own physical data centers and servers.  
	In simpler terms, cloud computing uses a network (most often, the internet) to connect users to a cloud platform where they request and access 
	rented computing services.

	There are three different cloud computing deployment models:
	\begin{itemize}
		\item \textbf{Public clouds} are run by third-party cloud service providers. They offer compute, storage, and network resources over the internet, 
		enabling companies to access shared on-demand resources based on their unique requirements and business goals.
		The most popular public cloud providers are Amazon, Google and Microsoft.
		\item \textbf{Private clouds} (also known as "on-premises" or "on-prem") are built, managed, and owned by a single organization 
		and privately hosted in their own data centers.
		They provide greater control, security, and management of data while still 
		enabling internal users to benefit from a shared pool of compute, storage, and network resources.
		\item \textbf{Hybrid clouds} are the mixture of using both public- and private clouds within the same organization.
	\end{itemize}

	There are three main types of cloud computing services:
	\begin{itemize}
		\item \textbf{Infrastructure as a service (IaaS)} offers on-demand access to IT infrastructure services, 
		including compute, storage, networking, and virtualization. It provides the highest level of control over your IT 
		resources and most closely resembles traditional on-premises IT resources.
		\item \textbf{Platform as a service (PaaS)} offers all the hardware and software resources needed for cloud 
		application development. With PaaS, companies can focus fully on application development without 
		the burden of managing and maintaining the underlying infrastructure.
		\item \textbf{Software as a service (SaaS)} delivers a full application stack as a service, from underlying 
		infrastructure to maintenance and updates to the app software itself. A SaaS solution is often an end-user application
		, where both the service and the infrastructure is managed and maintained by the cloud service provider.
	\end{itemize}

	\subsection{Cloud Native}
		Cloud native \cite{cloudnative} is the software approach of building, deploying, and managing modern applications 
		in cloud computing environments. Modern companies want to build highly scalable, flexible, and 
		resilient applications that they can update quickly to meet customer demands. 
		To do so, they use modern tools and techniques that inherently support application
		development on cloud infrastructure. These cloud-native technologies support fast 
		and frequent changes to applications without impacting service delivery, providing
		adopters with an innovative, competitive advantage.

	\subsection{Scaling}
		Scaling \cite{scaling} in cloud computing refers to the ability to increase or decrease IT resources as
		needed to meet changing demand. Scalability is one of the hallmarks of the cloud and the primary driver 
		of its exploding popularity with businesses. Scaling allows the businesses to pay only for the resources that
		they truly need.
		\begin{itemize}
			\item \textbf{Vertical scaling:} (also referred to as “scaling up” or “scaling down”) 
			You add or subtract power to an existing cloud server upgrading memory (RAM),
			storage or processing power (CPU).
			Usually this means that the scaling has an upper limit based on the capacity of the server
			or machine being scaled; scaling beyond that often requires downtime.
			\item \textbf{Horizontal scaling:} (also referred to as "scaling in" or "scaling out"), 
			You add more resources like servers to your system to spread out the workload across machines, 
			which in turn increases performance and storage capacity. 
			Horizontal scaling is especially important for businesses with high availability services requiring 
			minimal downtime.
		\end{itemize}

	\subsection{Load Balancing}
		Load balancing \cite{loadbalancing} is the practice of distributing computational workloads between two or more computers. 
		On the Internet, load balancing is often employed to divide network traffic among several servers. 
		This reduces the strain on each server and makes the servers more efficient, speeding up performance and 
		reducing latency. Load balancing is essential for most Internet applications to function properly. 
		By dividing user requests among multiple servers, user wait time is vastly cut down. 
		This results in a better user experience.

		Load balancing is handled by a tool or application called a load balancer. A load balancer can be 
		either hardware-based or software-based. 
		\begin{itemize}
			\item \textbf{Hardware load balancers} require the installation of a 
		dedicated load balancing device.
			\item \textbf{Software-based load balancers} can run on a server, on a virtual machine, 
		or in the cloud.
		\end{itemize}
		Content delivery networks (CDN) often include load balancing features.
		When a request arrives from a user, the load balancer assigns the request to a given server, 
		and this process repeats for each request. Load balancers determine which server should handle each request 
		based on a number of different algorithms. These algorithms fall into two main categories: static and dynamic.

		\begin{itemize}
			\item \textbf{Static load balancing} algorithms distribute workloads without taking into account the current 
			state of the system. A static load balancer will not be aware of which servers are performing 
			slowly and which servers are not being used enough. 
			Instead it assigns workloads based on a predetermined plan. 
			
			Static load balancing is quick to set up,
			but can result in inefficiencies. Round robin DNS and client-side random load balancing are 
			two common forms of static load balancing.
			\item \textbf{Dynamic load balancing} algorithms take the current availability, workload, 
			and health of each server into account. 
			They can shift traffic from overburdened or poorly performing servers to underutilized servers, 
			keeping the distribution even and efficient. However, dynamic load balancing is more difficult 
			to configure. A number of different factors play into server availability: 
			the health and overall capacity of each server, the size of the tasks being distributed, and so on.

			There are several types of dynamic load balancing algorithms, including least connection, weighted least connection, resource-based, and geolocation-based load balancing.
		\end{itemize}

\section{Amazon-related cloud technologies}
	\subsection{Amazon Web Services(AWS)}
	Amazon Web Services (AWS) is the public cloud offerring by the parent company Amazon. AWS has a vast offering
	of services, which help its end-users manage the infrastructures needed to host their own applications, 
	microservices, APIs and what not.
	AWS services are delivered to customers via a network of AWS server farms located throughout the world.

	In the following sections (2.14.2 - 2.14.6), I will go into detail what some of those services are and how
	they can be beneficial for the users of those services.

	\subsection{Amazon Elastic Compute Cloud (EC2)}

	\subsection{Amazon Elastic Container Service (ECS)}

	\subsection{Virtual Private Cloud (VPC)}

	\subsection{Application Load Balancing (ALB)}

	\subsection{Auto-Scaling}

\section{Containers}

	\subsection{Docker}

	\subsection{Docker compose}
